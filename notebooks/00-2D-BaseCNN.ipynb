{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65490a4e-5203-4837-acc2-7d45f99ee9b0",
   "metadata": {},
   "source": [
    "### TODOs\n",
    "\n",
    "Add Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890c327-af46-4b0a-ae4f-66b0b1eb80e1",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7beefc9-1c77-469d-aa86-0852ce4d8528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "\n",
    "import cv2\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69032e1-e9c2-42b2-ad4b-41828b74a756",
   "metadata": {},
   "source": [
    "#### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57758aa8-de49-474a-b7b9-23fc743a24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddc7dbb-4714-42e3-b8d7-ecfcf2ad6a1f",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e95b81-d9aa-489d-94c2-f2641cbd6121",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '..'\n",
    "\n",
    "# Pre-processing\n",
    "IMG_SIZE = 256\n",
    "SLICE_NUMBER = 50\n",
    "\n",
    "# Training\n",
    "N_EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe263350-abc7-47d3-a470-a57a7060b7e1",
   "metadata": {},
   "source": [
    "### 1. Load Data\n",
    "\n",
    "For each person, we were given *four* different MRI types: \n",
    "* FLAIR\n",
    "* T1w\n",
    "* T1wCE, and \n",
    "* T2w. \n",
    "\n",
    "We will create 2D \"images\" which are composed of each MRI type (4 sequences) where each sequence is composed of (middle) 50 (variable; slice_number) slices.\n",
    "\n",
    "Shape:\n",
    "Channel x Width x Height\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb4664-2143-4c1b-9f81-7a58d16edd73",
   "metadata": {},
   "source": [
    "### 1.1 Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cec6f51-ff10-46d5-a325-e90acd1e00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slices(mri_type, slice_number):\n",
    "    #print(f\"Length of folder: {len(mri_type)}\")\n",
    "    # Take slice_number slices from the middle\n",
    "    threshold = slice_number // 2\n",
    "    minimum_idx = len(mri_type)//2 - threshold if (len(mri_type)//2 - threshold) > 0 else 0\n",
    "    maximum_idx = len(mri_type)//2 + threshold  # maximum can exceed the index\n",
    "    #print(f\"Minimum {minimum_idx}\")\n",
    "    #print(f\"Maximum {maximum_idx}\")\n",
    "    # Create array which contains the images\n",
    "    mri_img = np.array([cv2.resize(cv2.imread(a, cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE)) for a in mri_type[minimum_idx:maximum_idx]]).T\n",
    "    # If less than slice_number slices, add slice_number - mri_img.shape[-1] images with only zero values\n",
    "    if mri_img.shape[-1] < slice_number:\n",
    "        #print(f\"Current slices: {mri_img.shape[-1]}\")\n",
    "        n_zero = slice_number - mri_img.shape[-1]\n",
    "        mri_img = np.concatenate((mri_img, np.zeros((IMG_SIZE, IMG_SIZE, n_zero))), axis = -1)\n",
    "    return mri_img\n",
    "    \n",
    "\n",
    "def load_images(scan_id, slice_number=SLICE_NUMBER):\n",
    "    # Ascending sort\n",
    "    flair = sorted(glob.glob(f\"{PATH}/train/{scan_id}/FLAIR/*.png\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    t1w = sorted(glob.glob(f\"{PATH}/train/{scan_id}/T1w/*.png\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    t1wce = sorted(glob.glob(f\"{PATH}/train/{scan_id}/T1wCE/*.png\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    t2w = sorted(glob.glob(f\"{PATH}/train/{scan_id}/T2w/*.png\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    \n",
    "    #print(f\"Scan id {scan_id}\")\n",
    "    flair_img = get_slices(flair, slice_number)\n",
    "    t1w_img = get_slices(t1w, slice_number)\n",
    "    t1wce_img = get_slices(t1wce, slice_number)\n",
    "    t2w_img = get_slices(t2w, slice_number)\n",
    "    \n",
    "    # Return \"3d\" image\n",
    "    # 0:50: Flair 2D images; 51:100: T1w 2D images; 101-150 T1wCE 2D images; 151-200 T2w 2D images\n",
    "    return np.concatenate((flair_img, t1w_img, t1wce_img, t2w_img), axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e375fe33-206e-47fa-87ee-39e5c68ac346",
   "metadata": {},
   "source": [
    "### 1.2 Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbf7314b-c6de-4490-b3cc-ca6a5295c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNADataset(Dataset):\n",
    "    def __init__(self, path='../', split = \"train\", validation_split = 0.2):\n",
    "        train_data = pd.read_csv(os.path.join(path, 'train_labels.csv'))\n",
    "        self.labels = {}\n",
    "        brats = list(train_data[\"BraTS21ID\"])\n",
    "        mgmt = list(train_data[\"MGMT_value\"])\n",
    "        for b, m in zip(brats, mgmt):\n",
    "            self.labels[str(b).zfill(5)] = m\n",
    "            \n",
    "        remove_ids = [\"00709\", \"00109\", \"00123\"]\n",
    "            \n",
    "        if split == \"valid\":\n",
    "            self.split = split\n",
    "            self.ids = [a.split(\"/\")[-1] for a in sorted(glob.glob((path + f\"/train/\" + \"/*\")), key=lambda f: int(re.sub('\\D', '', f)))]\n",
    "            self.ids = self.ids[:int(len(self.ids) * validation_split)] # first 20% as validation\n",
    "        elif split == \"train\":\n",
    "            self.split = split\n",
    "            self.ids = [a.split(\"/\")[-1] for a in sorted(glob.glob((path + f\"/train/\" + \"/*\")), key=lambda f: int(re.sub('\\D', '', f)))]\n",
    "            self.ids = self.ids[int(len(self.ids) * validation_split):] # last 80% as train\n",
    "        else:\n",
    "            self.split = split\n",
    "            self.ids = [a.split(\"/\")[-1] for a in sorted(glob.glob((path + f\"/train/\" + \"/*\")), key=lambda f: int(re.sub('\\D', '', f)))]\n",
    "        \n",
    "        self.ids = [id_ for id_ in self.ids if id_ not in remove_ids]            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        imgs = load_images(self.ids[idx])\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,) * (SLICE_NUMBER*4), (0.5,) * (SLICE_NUMBER*4))])\n",
    "        imgs = transform(imgs)\n",
    "\n",
    "        if self.split != \"test\":\n",
    "            label = self.labels[self.ids[idx]]\n",
    "            return torch.tensor(imgs, dtype = torch.float32), torch.tensor(label, dtype = torch.long)\n",
    "        else:\n",
    "            return torch.tensor(imgs, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f5408cd-a9f8-4736-b12e-e8679a79bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = RSNADataset()\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_ds = RSNADataset(split='valid')\n",
    "valid_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbc05117-0a72-4c3f-ae15-629c568e0bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the batch torch.Size([64, 200, 256, 256])\n",
      "Number of images/labels in the batch: 64\n",
      "Number of channels each image has: 200\n",
      "Size of each image is: 256x256\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_dl))\n",
    "print(f\"Shape of the batch {images.shape}\")\n",
    "print(f\"Number of images/labels in the batch: {images.shape[0]}\")\n",
    "print(f\"Number of channels each image has: {images.shape[1]}\")\n",
    "print(f\"Size of each image is: {images.shape[2]}x{images.shape[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd7879-f56f-4241-b841-9ee3e85d46c2",
   "metadata": {},
   "source": [
    "### 1.3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dabdf34-2396-421b-a86b-cf1e343ad20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GliobCNN(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(GliobCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=(SLICE_NUMBER*4), out_channels=64, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, stride=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Pooling layers\n",
    "        self.pooling1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.pooling2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.pooling3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(5832, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = F.relu(self.conv1(X))    # 64x254x254 (first dimension depends on SLICE_NUMBER; here, SLICE_NUMBER=50)\n",
    "        X = self.pooling1(X)         # 64x126x126\n",
    "        X = F.relu(self.conv2(X))    # 32x124x124\n",
    "        X = self.pooling2(X)         # 32x61x61\n",
    "        X = F.relu(self.conv3(X))    # 16x59x59\n",
    "        X = self.pooling3(X)         # 16x29x29\n",
    "        X = F.relu(self.conv4(X))    # 8x27x27\n",
    "        X = X.flatten(1)             # 5832\n",
    "        X = F.relu(self.fc1(X))      # 4096\n",
    "        X = F.relu(self.fc2(X))      # 2\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b3b9091-74b9-45dc-ab93-f5faaefe38e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GliobCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df09a3-54f9-4f8b-a083-122a50d8f565",
   "metadata": {},
   "source": [
    "### 1.4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7baecc1e-f75a-45f8-99a8-2541fe728813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(net, n_epochs, optimizer, loss_function, verbose=True):\n",
    "    # Store the losses for each epoch\n",
    "    loss_train_list = []\n",
    "    loss_valid_list = []\n",
    "\n",
    "    # Store the accuracy for each epoch\n",
    "    acc_train_list = []\n",
    "    acc_valid_list = []\n",
    "\n",
    "    # Iterate over the dataset n_epochs times\n",
    "    for epoch in range(n_epochs):\n",
    "        net.train()  # net.train() will notify all your layers that you are in training mode\n",
    "\n",
    "        train_loss = 0  # Training loss in epoch\n",
    "        num_train_correct  = 0\n",
    "        num_train_examples = 0\n",
    "\n",
    "        # For each batch, pass the training examples, calculate loss and gradients and optimize the parameters\n",
    "        for xb, yb in train_dl:\n",
    "            optimizer.zero_grad()  # zero_grad clears old gradients from the last step\n",
    "\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            y_hat = net(xb)  # Forward pass\n",
    "            loss = loss_function(y_hat, yb)  # Calculate Loss\n",
    "\n",
    "            loss.backward()  # Calculate the gradients (using backpropagation)\n",
    "            optimizer.step()  # # Optimize the parameters: opt.step() causes the optimizer to take a step based on the gradients of the parameters.\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            num_train_correct += (torch.max(y_hat, 1)[1] == yb).sum().item()\n",
    "            num_train_examples += xb.shape[0]\n",
    "\n",
    "        train_acc = num_train_correct / num_train_examples\n",
    "\n",
    "        valid_loss = 0  # Validation loss in epoch\n",
    "        num_val_correct  = 0\n",
    "        num_val_examples = 0\n",
    "\n",
    "        net.eval()  # net.eval() will notify all your layers that you are in evaluation mode\n",
    "        with torch.no_grad():\n",
    "            # Perform a prediction on the validation set  \n",
    "            for xb_valid, yb_valid in valid_dl:\n",
    "                xb_valid = xb_valid.to(device)\n",
    "                yb_valid = yb_valid.to(device)\n",
    "\n",
    "                y_hat = net(xb_valid)  # Forward pass\n",
    "                loss = loss_function(y_hat, yb_valid)  # Calculate Loss\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "                num_val_correct += (torch.max(y_hat, 1)[1] == yb_valid).sum().item()\n",
    "                num_val_examples += xb_valid.shape[0]\n",
    "\n",
    "        val_acc = num_val_correct / num_val_examples\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Train Loss (Negative Loss Likelihood) in epoch {epoch}: {train_loss:.2f}\")\n",
    "            print(f\"Validation Loss (Negative Loss Likelihood) in epoch {epoch}: {valid_loss:.2f}\")\n",
    "            print(f\"Train Accuracy in epoch {epoch}: {100 * (train_acc):.2f}\")\n",
    "            print(f\"Validation Accuracy in epoch {epoch}: {100 * (val_acc):.2f}\\n\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "        loss_train_list.append(train_loss)\n",
    "        loss_valid_list.append(valid_loss)\n",
    "        acc_train_list.append(100 * (train_acc))\n",
    "        acc_valid_list.append(100 * (val_acc))\n",
    "\n",
    "    return acc_train_list, acc_valid_list, loss_train_list, loss_valid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "749e8f68-d129-4924-a6a1-141bf929a5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss (Negative Loss Likelihood) in epoch 0: 5.55\n",
      "Validation Loss (Negative Loss Likelihood) in epoch 0: 1.39\n",
      "Train Accuracy in epoch 0: 46.90\n",
      "Validation Accuracy in epoch 0: 47.83\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Loss Function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.01)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "acc_train, acc_valid, loss_train, loss_valid = training(net=model, n_epochs=N_EPOCHS, optimizer=optimizer, loss_function=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a177609-6891-4f94-ab65-544ec174b088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [pipenv: Kaggle]",
   "language": "python",
   "name": "kaggle_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
