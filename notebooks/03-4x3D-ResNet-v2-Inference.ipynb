{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c4df0b-8f20-49bc-9e75-fcc5e3985edd",
   "metadata": {},
   "source": [
    "# Baseline - 3D CNN - 4 Classifiers - Inference\n",
    "\n",
    "In this notebook, we will train **four** 3D convolutional neural networks and combine their predictions in order to recognize a brain tumor.\n",
    "\n",
    "For each case (i.e., person) we know whether the person suffered from cancer (1) or not (0). Each independent case has a dedicated folder identified by a five-digit number. Within each of these “case” folders, there are four sub-folders, each of them corresponding to an MRI scan, The MRI scans include:\n",
    "\n",
    "* Fluid Attenuated Inversion Recovery (FLAIR)\n",
    "* T1-weighted pre-contrast (T1w)\n",
    "* T1-weighted post-contrast (T1Gd)\n",
    "* T2-weighted (T2)\n",
    "\n",
    "I am using the datas set created by [Jonathan Besomi](https://www.kaggle.com/c/rsna-miccai-brain-tumor-radiogenomic-classification/discussion/253000#1388021). Many thanks for creating the data set!\n",
    "\n",
    "**3D images**\n",
    "* each image has the following shape: Channel x Width x Height x Depth (i.e., 1 x Width x Height x Depth)\n",
    "* *depth:* the depth represents the depth or the number of slices. (I tried various values >= 30. If for a given id, the depth <= the given value (e.g., 30), I replaced the missing depth slices with zero matrices (see [Zabir Al Nazi Nabil](https://www.kaggle.com/furcifer/torch-efficientnet3d-for-mri-no-train)))\n",
    "* added some albumentation such as CLAHE, brightness, and CoarseDropout for the training images\n",
    "* removed black pixels (see [Zabir Al Nazi Nabil](https://www.kaggle.com/furcifer/torch-efficientnet3d-for-mri-no-train))\n",
    "\n",
    "**3D CNN** <br>\n",
    "TODO: ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890c327-af46-4b0a-ae4f-66b0b1eb80e1",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7beefc9-1c77-469d-aa86-0852ce4d8528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "\n",
    "import cv2\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69032e1-e9c2-42b2-ad4b-41828b74a756",
   "metadata": {},
   "source": [
    "#### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57758aa8-de49-474a-b7b9-23fc743a24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.ma^nual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf734f-94b6-40e9-9657-1bcc4b0aa83b",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea5e710-dcb8-4e43-86b2-c86a0ade95fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../input/rsna-miccai-brain-tumor-radiogenomic-classification'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddc7dbb-4714-42e3-b8d7-ecfcf2ad6a1f",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e95b81-d9aa-489d-94c2-f2641cbd6121",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    # Pre-processing\n",
    "    SLICE_NUMBER = 50, # >= 30\n",
    "    REMOVE_BLACK_BOUNDARIES = True,\n",
    "    DICOM=False,\n",
    "    # Albumentation\n",
    "    RRC_SIZE = 256,\n",
    "    RRC_MIN_SCALE = 0.85,\n",
    "    RRC_RATIO = (1., 1.),\n",
    "    CLAHE_CLIP_LIMIT = 2.0,\n",
    "    CLAHE_TILE_GRID_SIZE = (8, 8),\n",
    "    CLAHE_PROB = 0.50,\n",
    "    BRIGHTNESS_LIMIT = (-0.2,0.2),\n",
    "    BRIGHTNESS_PROB = 0.40,\n",
    "    HUE_SHIFT = (-15, 15),\n",
    "    SAT_SHIFT = (-15, 15),\n",
    "    VAL_SHIFT = (-15, 15),\n",
    "    HUE_PROB = 0.64,\n",
    "    COARSE_MAX_HOLES = 16,\n",
    "    COARSE_PROB = 0.7,\n",
    "    # Training\n",
    "    N_EPOCHS = 15,\n",
    "    BATCH_SIZE = 8,\n",
    "    LEARNING_RATE = 0.001,\n",
    "    WEIGHT_DECAY = 0.02,\n",
    "    # Logging\n",
    "    VERBOSE = False,\n",
    "    MODELNAME = \"02-3D-4-ResNet18\"\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6ace1-a624-45dd-a40a-18dcd36d340b",
   "metadata": {},
   "source": [
    "#### wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a67c0-30f1-4072-88e8-d89c4f346ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"01-3D-4-ResNet\"\n",
    "wandb.login()\n",
    "run = wandb.init(project='rsna-miccai', config=config)\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe263350-abc7-47d3-a470-a57a7060b7e1",
   "metadata": {},
   "source": [
    "### 1. Load Data\n",
    "\n",
    "To create a 3D image, we will pick SLICE_NUMBER middle slices from each of the four MRI types (i.e., FLAIR, T1w, T1wCE, T2w). For example, if we set *SLICE_NUMBER=30*, each 3D image will have the shape: 1 x Width x Height x 30. Further, one image contains only the images from *one* MRI type as opposed to other notebooks where all MRI types are combined in a single image.\n",
    "\n",
    "* If for a given MRI type, the number of images < SLICE_Number, than we will \"fill up\" the remaining *number_of_images - SLICE_NUMBER* slices with all black images\n",
    "* We removed black pixels (for more information, see [here](https://www.kaggle.com/furcifer/torch-efficientnet3d-for-mri-no-train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb4664-2143-4c1b-9f81-7a58d16edd73",
   "metadata": {},
   "source": [
    "### 1.1 Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2fba5-e062-4ab1-8b9e-ca72e9b36732",
   "metadata": {},
   "source": [
    "#### 1.1.1 Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c02d3-d25b-4c3f-95e5-545894a0a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.RandomResizedCrop(\n",
    "        config.RRC_SIZE, config.RRC_SIZE,            \n",
    "        scale=(config.RRC_MIN_SCALE, 1.0),\n",
    "        ratio=config.RRC_RATIO,\n",
    "        p=1.0\n",
    "    ),\n",
    "    A.CLAHE(\n",
    "        clip_limit=config.CLAHE_CLIP_LIMIT,\n",
    "        tile_grid_size=config.CLAHE_TILE_GRID_SIZE,\n",
    "        p=config.CLAHE_PROB\n",
    "    ),\n",
    "    A.RandomBrightnessContrast(\n",
    "        brightness_limit=config.BRIGHTNESS_LIMIT,\n",
    "        p=config.BRIGHTNESS_PROB\n",
    "    ),\n",
    "    A.HueSaturationValue(\n",
    "        hue_shift_limit=config.HUE_SHIFT, \n",
    "        sat_shift_limit=config.SAT_SHIFT, \n",
    "        val_shift_limit=config.VAL_SHIFT, \n",
    "        p=config.HUE_PROB\n",
    "    ),\n",
    "    A.CoarseDropout(\n",
    "        max_holes=config.COARSE_MAX_HOLES,\n",
    "        p=config.COARSE_PROB\n",
    "    ),\n",
    "    A.Normalize(\n",
    "        mean=(123.675),\n",
    "        std=(58.39),\n",
    "        max_pixel_value=255.0,\n",
    "        always_apply=True\n",
    "    )\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    A.RandomResizedCrop( \n",
    "        config.RRC_SIZE, config.RRC_SIZE,            \n",
    "        scale=(config.RRC_MIN_SCALE, 1.0),\n",
    "        ratio=config.RRC_RATIO,\n",
    "        p=1.0\n",
    "    ),\n",
    "    A.Normalize(\n",
    "        mean=(123.675),\n",
    "        std=(58.39),\n",
    "        max_pixel_value=255.0,\n",
    "        always_apply=True\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7aedeb-df7f-45a4-9646-97bde2c10c19",
   "metadata": {},
   "source": [
    "#### 1.1.2 Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec6f51-ff10-46d5-a325-e90acd1e00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicom_2_image(file, voi_lut=True, fix_monochrome=True):\n",
    "    dicom = pydicom.read_file(file)\n",
    "    # VOI LUT (if available by DICOM device) is used to\n",
    "    # transform raw DICOM data to \"human-friendly\" view\n",
    "    if voi_lut:\n",
    "        img = apply_voi_lut(dicom.pixel_array, dicom)\n",
    "    else:\n",
    "        img = dicom.pixel_array\n",
    "    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "        img = np.amax(img) - img\n",
    "    \n",
    "    img = img - np.min(img)\n",
    "    img = img / np.max(img)\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "def get_3d_image(mri_type, aug, dicom):\n",
    "    if config.VERBOSE:\n",
    "        print(f\"Length of folder: {len(mri_type)}\")\n",
    "    # Take SLICE_NUMBER slices from the middle\n",
    "    threshold = config.SLICE_NUMBER // 2\n",
    "    minimum_idx = len(mri_type)//2 - threshold if (len(mri_type)//2 - threshold) > 0 else 0\n",
    "    maximum_idx = len(mri_type)//2 + threshold  # maximum can exceed the index\n",
    "    if config.VERBOSE:\n",
    "        print(f\"Minimum {minimum_idx}\")\n",
    "        print(f\"Maximum {maximum_idx}\")\n",
    "    # Create array which contains the images\n",
    "    mri_img = []\n",
    "    for file in mri_type[minimum_idx:maximum_idx]:\n",
    "        if dicom:\n",
    "            img = dicom_2_image(file)\n",
    "        else:\n",
    "            img = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
    "        if config.REMOVE_BLACK_BOUNDARIES:\n",
    "            (x, y) = np.where(img > 0)\n",
    "            if len(x) > 0 and len(y) > 0:\n",
    "                x_mn = np.min(x)\n",
    "                x_mx = np.max(x)\n",
    "                y_mn = np.min(y)\n",
    "                y_mx = np.max(y)\n",
    "                if (x_mx - x_mn) > 10 and (y_mx - y_mn) > 10:\n",
    "                    img = img[:,np.min(y):np.max(y)]\n",
    "        if aug:\n",
    "            transformed = train_transform(image=img)\n",
    "            img = transformed[\"image\"]\n",
    "        else:\n",
    "            transformed = valid_transform(image=img)\n",
    "            img = transformed[\"image\"]\n",
    "        mri_img.append(np.array(img))\n",
    "    mri_img = np.array(mri_img).T\n",
    "    # If less than SLICE_NUMBER slices, add SLICE_NUMBER - mri_img.shape[-1] images with only zero values\n",
    "    if mri_img.shape[-1] < config.SLICE_NUMBER:\n",
    "        if config.VERBOSE:\n",
    "            print(f\"Current slices: {mri_img.shape[-1]}\")\n",
    "        n_zero = config.SLICE_NUMBER - mri_img.shape[-1]\n",
    "        mri_img = np.concatenate((mri_img, np.zeros((config.RRC_SIZE, config.RRC_SIZE, n_zero))), axis = -1)\n",
    "    if config.VERBOSE:\n",
    "        print(f\"Shape of mri_img: {mri_img.shape}\")\n",
    "    return mri_img\n",
    "    \n",
    "\n",
    "    \n",
    "def load_images(scan_id, mri_type, aug=True, split=\"train\", dicom=False):\n",
    "    file_ext = \"png\"\n",
    "    if dicom:\n",
    "        file_ext = \"dcm\"\n",
    "    if config.VERBOSE:\n",
    "        print(f\"Scan id {scan_id}\")\n",
    "        \n",
    "    # Ascending sort\n",
    "    if mri_type == \"FLAIR\":\n",
    "        flair = sorted(glob.glob(f\"{PATH}/{split}/{scan_id}/FLAIR/*.{file_ext}\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        img = get_3d_image(flair, aug, dicom)\n",
    "    elif mri_type == \"T1w\":\n",
    "        t1w = sorted(glob.glob(f\"{PATH}/{split}/{scan_id}/T1w/*.{file_ext}\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        img = get_3d_image(t1w, aug, dicom)\n",
    "    elif mri_type == \"T1wCE\":\n",
    "        t1wce = sorted(glob.glob(f\"{PATH}/{split}/{scan_id}/T1wCE/*.{file_ext}\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        img = get_3d_image(t1wce, aug, dicom)\n",
    "    else:\n",
    "        t2w = sorted(glob.glob(f\"{PATH}/{split}/{scan_id}/T2w/*.{file_ext}\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        img = get_3d_image(t2w, aug, dicom)\n",
    "    \n",
    "    # Return 3D image: ChannelsxWidthxHeightxDepth\n",
    "    img = img.reshape((1,config.RRC_SIZE,config.RRC_SIZE,config.SLICE_NUMBER))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a29f7-6588-4f19-8bef-f246a3dace77",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "The first row represents images with augmentation and the second row images without augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76f9233-737c-4bb0-888a-35733ddea734",
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_id = \"00000\"\n",
    "slice_nb = 0\n",
    "\n",
    "figure, axes = plt.subplots(2,4, figsize=(15,15))\n",
    "\n",
    "flair_3d_image = load_images(scan_id, \"FLAIR\", aug=True)\n",
    "flair_3d_image_fs = flair_3d_image[0,:,:,slice_nb]\n",
    "t1w_3d_image = load_images(scan_id, \"T1w\", aug=True)\n",
    "t1w_3d_image_fs = t1w_3d_image[0,:,:,slice_nb]\n",
    "t1wce_3d_image = load_images(scan_id, \"T1wCE\", aug=True)\n",
    "t1wce_3d_image_fs = t1wce_3d_image[0,:,:,slice_nb]\n",
    "t2w_3d_image = load_images(scan_id, \"T2w\", aug=True)\n",
    "t2w_3d_image_fs = t2w_3d_image[0,:,:,slice_nb]\n",
    "\n",
    "im = axes[0,0].imshow(flair_3d_image_fs)\n",
    "im = axes[0,1].imshow(t1w_3d_image_fs)\n",
    "im = axes[0,2].imshow(t1wce_3d_image_fs)\n",
    "im = axes[0,3].imshow(t2w_3d_image_fs)\n",
    "\n",
    "for ax in axes[0]:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "flair_3d_image = load_images(scan_id, \"FLAIR\", aug=False)\n",
    "flair_3d_image_fs = flair_3d_image[0,:,:,slice_nb]\n",
    "t1w_3d_image = load_images(scan_id, \"T1w\", aug=False)\n",
    "t1w_3d_image_fs = t1w_3d_image[0,:,:,slice_nb]\n",
    "t1wce_3d_image = load_images(scan_id, \"T1wCE\", aug=False)\n",
    "t1wce_3d_image_fs = t1wce_3d_image[0,:,:,slice_nb]\n",
    "t2w_3d_image = load_images(scan_id, \"T2w\", aug=False)\n",
    "t2w_3d_image_fs = t2w_3d_image[0,:,:,slice_nb]\n",
    "\n",
    "im = axes[1,0].imshow(flair_3d_image_fs)\n",
    "im = axes[1,1].imshow(t1w_3d_image_fs)\n",
    "im = axes[1,2].imshow(t1wce_3d_image_fs)\n",
    "im = axes[1,3].imshow(t2w_3d_image_fs)\n",
    "\n",
    "for ax in axes[1]:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e375fe33-206e-47fa-87ee-39e5c68ac346",
   "metadata": {},
   "source": [
    "### 1.2 Dataset and Dataloader\n",
    "\n",
    "Create a PyTorch Dataset and DataLoader **for each** MRI type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2679b9c-dcd6-435a-a228-45da0d9896b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ids and labels and make a stratified 80:20 split\n",
    "df_test = pd.read_csv(f\"{PATH}/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf7314b-c6de-4490-b3cc-ca6a5295c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNADataset(Dataset):\n",
    "    def __init__(self, ids, labels, mri_type=\"FLAIR\", split=\"train\", dicom=False, label_smoothing=0.0):\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "        self.mri_type = mri_type\n",
    "        self.split = split\n",
    "        self.dicom = dicom\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "        if split == \"train\":\n",
    "            remove_ids = [709, 109, 123]\n",
    "            self.ids = [id_ for id_ in self.ids if id_ not in remove_ids]  \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.ids[idx]\n",
    "        patient_id = str(patient_id).zfill(5)\n",
    "        if self.split == \"train\":\n",
    "            imgs = load_images(patient_id, self.mri_type, aug=True, dicom=self.dicom)\n",
    "        elif self.split == \"valid\":\n",
    "            imgs = load_images(patient_id, self.mri_type, aug=False, dicom=self.dicom)\n",
    "        else:\n",
    "            imgs = load_images(patient_id, self.mri_type, aug=False, split=self.split, dicom=self.dicom)\n",
    "        # Normalize\n",
    "        imgs = imgs - imgs.min()\n",
    "        imgs = (imgs + 1e-5) / (imgs.max() - imgs.min() + 1e-5)\n",
    "\n",
    "        if self.split != \"test\":\n",
    "            label = abs(self.labels[idx] - self.label_smoothing)\n",
    "            return torch.tensor(imgs, dtype = torch.float32), torch.tensor(label, dtype = torch.long)\n",
    "        else:\n",
    "            return torch.tensor(imgs, dtype = torch.float32), torch.tensor(self.ids[idx], dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f695b23-3d99-40c5-a637-42ffb88d3171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAIR DataLoader\n",
    "test_flair_ds = RSNADataset(df_test[\"BraTS21ID\"].to_numpy(), df_test[\"MGMT_value\"].to_numpy(), mri_type=\"FLAIR\", split='test')\n",
    "test_flair_dl = DataLoader(test_flair_ds, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# T1w DataLoader\n",
    "test_t1w_ds = RSNADataset(df_test[\"BraTS21ID\"].to_numpy(), df_test[\"MGMT_value\"].to_numpy(), mri_type=\"T1w\", split='test')\n",
    "test_t1w_dl = DataLoader(test_t1w_ds, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# T1wCE DataLoader\n",
    "test_t1wce_ds = RSNADataset(df_test[\"BraTS21ID\"].to_numpy(), df_test[\"MGMT_value\"].to_numpy(), mri_type=\"T1wCE\", split='test')\n",
    "test_t1wce_dl = DataLoader(test_t1wce_ds, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# T2w DataLoader\n",
    "test_t2w_ds = RSNADataset(df_test[\"BraTS21ID\"].to_numpy(), df_test[\"MGMT_value\"].to_numpy(), mri_type=\"T2w\", split='test')\n",
    "test_t2w_dl = DataLoader(test_t2w_ds, batch_size=config.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc05117-0a72-4c3f-ae15-629c568e0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_flair_dl))\n",
    "print(f\"Shape of the batch {images.shape}\")\n",
    "print(f\"Batch size: {images.shape[0]}\")\n",
    "print(f\"Number of channels each image has: {images.shape[1]}\")\n",
    "print(f\"Size of each image is: {images.shape[2]}x{images.shape[3]}\")\n",
    "print(f\"Depth of each channel/sequence: {images.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd7879-f56f-4241-b841-9ee3e85d46c2",
   "metadata": {},
   "source": [
    "### 1.3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32403aea-b7a9-4047-8f9b-2e302e9ea0e7",
   "metadata": {},
   "source": [
    "#### 1.3.1 3D ResNet18\n",
    "\n",
    "For more information, see [MedicalNet](https://github.com/Tencent/MedicalNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd327d83-3b1f-48b2-b7dc-2a8531124fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "def conv1x1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv3x3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv1x1x1(in_planes, planes)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.conv2 = conv3x3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.conv3 = conv1x1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, block_inplanes, n_input_channels=1, conv1_t_size=7, conv1_t_stride=1, no_max_pool=False,\n",
    "                 shortcut_type='B', widen_factor=1.0, n_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # For wider ResNets only\n",
    "        block_inplanes = [int(x * widen_factor) for x in block_inplanes]\n",
    "\n",
    "        self.in_planes = block_inplanes[0]  # 64\n",
    "        self.no_max_pool = no_max_pool\n",
    "\n",
    "        # First Block: Convolutional Layer + Batch Normalization + ReLu + Max Pooling\n",
    "        self.conv1 = nn.Conv3d(n_input_channels, self.in_planes, kernel_size=(conv1_t_size, 7, 7), stride=(conv1_t_stride, 2, 2),\n",
    "                               padding=(conv1_t_size // 2, 3, 3), bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(self.in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "        # Example ResNet18: Each layer consists of two residual blocks and two residual connections\n",
    "        self.layer1 = self._make_layer(block, block_inplanes[0], layers[0], shortcut_type)\n",
    "        self.layer2 = self._make_layer(block, block_inplanes[1], layers[1], shortcut_type, stride=2)\n",
    "        self.layer3 = self._make_layer(block, block_inplanes[2], layers[2], shortcut_type, stride=2)\n",
    "        self.layer4 = self._make_layer(block, block_inplanes[3], layers[3], shortcut_type, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(block_inplanes[3] * block.expansion, n_classes)\n",
    "\n",
    "        # Initialization\n",
    "        for m in self.modules():\n",
    "            # He initialization\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _downsample_basic_block(self, x, planes, stride):\n",
    "        out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
    "        zero_pads = torch.zeros(out.size(0), planes - out.size(1), out.size(2), out.size(3), out.size(4))\n",
    "        if isinstance(out.data, torch.cuda.FloatTensor):\n",
    "            zero_pads = zero_pads.cuda()\n",
    "\n",
    "        out = torch.cat([out.data, zero_pads], dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            if shortcut_type == 'A':\n",
    "                downsample = partial(self._downsample_basic_block, planes=planes * block.expansion, stride=stride)\n",
    "            else:\n",
    "                downsample = nn.Sequential(conv1x1x1(self.in_planes, planes * block.expansion, stride), \n",
    "                                           nn.BatchNorm3d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(in_planes=self.in_planes, planes=planes, stride=stride, downsample=downsample))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        if not self.no_max_pool:\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18455c3-40a0-4843-aa96-00e044538732",
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_model = ResNet(Bottleneck, [3, 4, 6, 3], [64, 128, 256, 512])\n",
    "flair_model.load_state_dict(torch.load(\"../models/01-3D-4-ResNet18-pretrained-FLAIR-roc-0.68.pt\"))\n",
    "flair_model.to(device)\n",
    "t1w_model = ResNet(Bottleneck, [3, 4, 6, 3], [64, 128, 256, 512])\n",
    "t1w_model.load_state_dict(torch.load(\"../models/01-3D-4-ResNet18-pretrained-T1w-roc-0.61.pt\"))\n",
    "t1w_model.to(device)\n",
    "t1wce_model = ResNet(Bottleneck, [3, 4, 6, 3], [64, 128, 256, 512])\n",
    "t1wce_model.load_state_dict(torch.load(\"../models/01-3D-4-ResNet18-pretrained-T1wCE-roc-0.59.pt\"))\n",
    "t1wce_model.to(device)\n",
    "t2w_model = ResNet(Bottleneck, [3, 4, 6, 3], [64, 128, 256, 512])\n",
    "t2w_model.load_state_dict(torch.load(\"../models/01-3D-4-ResNet18-pretrained-T2w-roc-0.71.pt\"))\n",
    "t2w_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df09a3-54f9-4f8b-a083-122a50d8f565",
   "metadata": {},
   "source": [
    "### 1.4 Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb1a444-6f04-4250-9bd1-9059855f8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(net, test_dl):\n",
    "    y_hats = None\n",
    "    idx_list = []\n",
    "    \n",
    "    net.eval()\n",
    "    for xb, idxb in tqdm(test_dl, desc=\"Testing\"):\n",
    "        xb = xb.to(device)\n",
    "        y_hat = net(xb)\n",
    "        y_hat = F.softmax(y_hat)[:,1].cpu().detach().numpy()\n",
    "        if y_hats is None:\n",
    "            y_hats = y_hat\n",
    "            idx_list = idxb.numpy()\n",
    "        else:\n",
    "            y_hats = np.concatenate((y_hats, y_hat), axis=0)\n",
    "            idx_list = np.concatenate((idx_list, idxb.numpy()), axis=0)\n",
    "    \n",
    "    return y_hats, idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85b109-5ef7-475f-b4a4-5180a87e6f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_flair_hats, idx_list = submission(flair_model, test_flair_dl)\n",
    "y_t1w_hats, idx_list = submission(t1w_model, test_t1w_dl)\n",
    "y_t1wce_hats, idx_list = submission(t1wce_model, test_t1wce_dl)\n",
    "y_t2w_hats, idx_list = submission(t2w_model, test_t2w_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a56a7-1606-4062-bbca-1e93d81e6e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'BraTS21ID': idx_list, 'FLAIR': y_flair_hats, 'T1w': y_t1w_hats, 'T1wCE': y_t1wce_hats,\n",
    "                   'T2w': y_t2w_hats})\n",
    "df[\"MGMT_value\"] = (df[\"FLAIR\"] + df[\"T1w\"] + df[\"T1wCE\"] + df[\"T2w\"]) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21521e51-4b8a-4530-8013-421f88b211df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = df[[\"BraTS21ID\", \"MGMT_value\"]]\n",
    "df_submission.to_csv(\"submission.csv\", index=False)\n",
    "df_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4f297-3162-44e2-a3dc-9de25b3b560b",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [pipenv: Kaggle]",
   "language": "python",
   "name": "kaggle_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
