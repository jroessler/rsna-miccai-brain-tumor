{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c4df0b-8f20-49bc-9e75-fcc5e3985edd",
   "metadata": {},
   "source": [
    "# Baseline - 3D CNN - 4 Classifiers\n",
    "\n",
    "In this notebook, we will train **four** 3D convolutional neural networks and combine their predictions in order to recognize a brain tumor.\n",
    "\n",
    "For each case (i.e., person) we know whether the person suffered from cancer (1) or not (0). Each independent case has a dedicated folder identified by a five-digit number. Within each of these “case” folders, there are four sub-folders, each of them corresponding to an MRI scan, The MRI scans include:\n",
    "\n",
    "* Fluid Attenuated Inversion Recovery (FLAIR)\n",
    "* T1-weighted pre-contrast (T1w)\n",
    "* T1-weighted post-contrast (T1Gd)\n",
    "* T2-weighted (T2)\n",
    "\n",
    "I am using the datas set created by [Jonathan Besomi](https://www.kaggle.com/c/rsna-miccai-brain-tumor-radiogenomic-classification/discussion/253000#1388021). Many thanks for creating the data set!\n",
    "\n",
    "**3D images**\n",
    "* each image has the following shape: Channel x Width x Height x Depth (i.e., 1 x Width x Height x Depth)\n",
    "* *depth:* the depth represents the depth or the number of slices. (I tried various values >= 30. If for a given id, the depth <= the given value (e.g., 30), I replaced the missing depth slices with zero matrices (see [Zabir Al Nazi Nabil](https://www.kaggle.com/furcifer/torch-efficientnet3d-for-mri-no-train)))\n",
    "* added some albumentation such as CLAHE, brightness, and CoarseDropout for the training images\n",
    "* removed black pixels (see [Zabir Al Nazi Nabil](https://www.kaggle.com/furcifer/torch-efficientnet3d-for-mri-no-train))\n",
    "\n",
    "**3D CNN** <br>\n",
    "For each MRI type (i.e., FLAIR, T1w, T1wCE, T2w), we will creat & train a 3D CNN model, respectively with the following architecture\n",
    "* 4x Conv3d layers\n",
    "* 3x MaxPool3d layers\n",
    "* 1x AdaptiveMaxPool3d layers\n",
    "* 2x fc layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890c327-af46-4b0a-ae4f-66b0b1eb80e1",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7beefc9-1c77-469d-aa86-0852ce4d8528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "\n",
    "import cv2\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69032e1-e9c2-42b2-ad4b-41828b74a756",
   "metadata": {},
   "source": [
    "#### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57758aa8-de49-474a-b7b9-23fc743a24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddc7dbb-4714-42e3-b8d7-ecfcf2ad6a1f",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e95b81-d9aa-489d-94c2-f2641cbd6121",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '..'\n",
    "\n",
    "config = dict(\n",
    "    # Pre-processing\n",
    "    SLICE_NUMBER = 50, # >= 30\n",
    "    REMOVE_BLACK_BOUNDARIES = True,\n",
    "    DATASET='PNG',\n",
    "    \n",
    "    # Albumentation\n",
    "    RRC_SIZE = 256,\n",
    "    RRC_MIN_SCALE = 0.85,\n",
    "    RRC_RATIO = (1., 1.),\n",
    "    CLAHE_CLIP_LIMIT = 2.0,\n",
    "    CLAHE_TILE_GRID_SIZE = (8, 8),\n",
    "    CLAHE_PROB = 0.50,\n",
    "    BRIGHTNESS_LIMIT = (-0.2,0.2),\n",
    "    BRIGHTNESS_PROB = 0.40,\n",
    "    HUE_SHIFT = (-15, 15),\n",
    "    SAT_SHIFT = (-15, 15),\n",
    "    VAL_SHIFT = (-15, 15),\n",
    "    HUE_PROB = 0.64,\n",
    "    COARSE_MAX_HOLES = 16,\n",
    "    COARSE_PROB = 0.7,\n",
    "    \n",
    "    # Training\n",
    "    N_EPOCHS = 10,\n",
    "    BATCH_SIZE = 8,\n",
    "    LEARNING_RATE = 0.01,\n",
    "    WEIGHT_DECAY = 0.02,\n",
    "    \n",
    "    # Logging\n",
    "    VERBOSE = False,\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6ace1-a624-45dd-a40a-18dcd36d340b",
   "metadata": {},
   "source": [
    "#### wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a67c0-30f1-4072-88e8-d89c4f346ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"01-3D-4-BaseCNN\"\n",
    "wandb.login()\n",
    "run = wandb.init(project='rsna-miccai', config=config)\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe263350-abc7-47d3-a470-a57a7060b7e1",
   "metadata": {},
   "source": [
    "### 1. Load Data\n",
    "\n",
    "To create a 3D image, we will pick SLICE_NUMBER middle slices from each of the four MRI types (i.e., FLAIR, T1w, T1wCE, T2w). For example, if we set *SLICE_NUMBER=30*, each 3D image will have the shape: 1 x Width x Height x 30. Further, one image contains only the images from *one* MRI type as opposed to other notebooks where all MRI types are combined in a single image.\n",
    "\n",
    "* If for a given MRI type, the number of images < SLICE_Number, than we will \"fill up\" the remaining *number_of_images - SLICE_NUMBER* slices with all black images\n",
    "* We removed black pixels (for more information, see [here](https://www.kaggle.com/furcifer/torch-efficientnet3d-for-mri-no-train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb4664-2143-4c1b-9f81-7a58d16edd73",
   "metadata": {},
   "source": [
    "### 1.1 Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2fba5-e062-4ab1-8b9e-ca72e9b36732",
   "metadata": {},
   "source": [
    "#### 1.1.1 Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c02d3-d25b-4c3f-95e5-545894a0a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.RandomResizedCrop(\n",
    "        config.RRC_SIZE, config.RRC_SIZE,            \n",
    "        scale=(config.RRC_MIN_SCALE, 1.0),\n",
    "        ratio=config.RRC_RATIO,\n",
    "        p=1.0\n",
    "    ),\n",
    "    A.CLAHE(\n",
    "        clip_limit=config.CLAHE_CLIP_LIMIT,\n",
    "        tile_grid_size=config.CLAHE_TILE_GRID_SIZE,\n",
    "        p=config.CLAHE_PROB\n",
    "    ),\n",
    "    A.RandomBrightnessContrast(\n",
    "        brightness_limit=config.BRIGHTNESS_LIMIT,\n",
    "        p=config.BRIGHTNESS_PROB\n",
    "    ),\n",
    "    A.HueSaturationValue(\n",
    "        hue_shift_limit=config.HUE_SHIFT, \n",
    "        sat_shift_limit=config.SAT_SHIFT, \n",
    "        val_shift_limit=config.VAL_SHIFT, \n",
    "        p=config.HUE_PROB\n",
    "    ),\n",
    "    A.CoarseDropout(\n",
    "        max_holes=config.COARSE_MAX_HOLES,\n",
    "        p=config.COARSE_PROB),\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    A.RandomResizedCrop( \n",
    "        config.RRC_SIZE, config.RRC_SIZE,            \n",
    "        scale=(config.RRC_MIN_SCALE, 1.0),\n",
    "        ratio=config.RRC_RATIO,\n",
    "        p=1.0\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7aedeb-df7f-45a4-9646-97bde2c10c19",
   "metadata": {},
   "source": [
    "#### 1.1.2 Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec6f51-ff10-46d5-a325-e90acd1e00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slices(mri_type, aug):\n",
    "    if config.VERBOSE:\n",
    "        print(f\"Length of folder: {len(mri_type)}\")\n",
    "    # Take SLICE_NUMBER slices from the middle\n",
    "    threshold = config.SLICE_NUMBER // 2\n",
    "    minimum_idx = len(mri_type)//2 - threshold if (len(mri_type)//2 - threshold) > 0 else 0\n",
    "    maximum_idx = len(mri_type)//2 + threshold  # maximum can exceed the index\n",
    "    if config.VERBOSE:\n",
    "        print(f\"Minimum {minimum_idx}\")\n",
    "        print(f\"Maximum {maximum_idx}\")\n",
    "    # Create array which contains the images\n",
    "    mri_img = []\n",
    "    for file in mri_type[minimum_idx:maximum_idx]:\n",
    "        img = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
    "        if config.REMOVE_BLACK_BOUNDARIES:\n",
    "            (x, y) = np.where(img > 0)\n",
    "            if len(x) > 0 and len(y) > 0:\n",
    "                x_mn = np.min(x)\n",
    "                x_mx = np.max(x)\n",
    "                y_mn = np.min(y)\n",
    "                y_mx = np.max(y)\n",
    "                if (x_mx - x_mn) > 10 and (y_mx - y_mn) > 10:\n",
    "                    img = img[:,np.min(y):np.max(y)]\n",
    "        if aug:\n",
    "            transformed = train_transform(image=img)\n",
    "            img = transformed[\"image\"]\n",
    "        else:\n",
    "            transformed = valid_transform(image=img)\n",
    "            img = transformed[\"image\"]\n",
    "        mri_img.append(np.array(img))\n",
    "    mri_img = np.array(mri_img).T\n",
    "    # If less than SLICE_NUMBER slices, add SLICE_NUMBER - mri_img.shape[-1] images with only zero values\n",
    "    if mri_img.shape[-1] < config.SLICE_NUMBER:\n",
    "        if config.VERBOSE:\n",
    "            print(f\"Current slices: {mri_img.shape[-1]}\")\n",
    "        n_zero = config.SLICE_NUMBER - mri_img.shape[-1]\n",
    "        mri_img = np.concatenate((mri_img, np.zeros((config.RRC_SIZE, config.RRC_SIZE, n_zero))), axis = -1)\n",
    "    if config.VERBOSE:\n",
    "        print(f\"Shape of mri_img: {mri_img.shape}\")\n",
    "    return mri_img\n",
    "    \n",
    "\n",
    "def load_images(scan_id, mri_type, aug=True):\n",
    "    if config.VERBOSE:\n",
    "        print(f\"Scan id {scan_id}\")\n",
    "        \n",
    "    # Ascending sort\n",
    "    if mri_type == \"FLAIR\":\n",
    "        flair = sorted(glob.glob(f\"{PATH}/train/{scan_id}/FLAIR/*.png\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        img = get_slices(flair, aug)\n",
    "    elif mri_type == \"T1w\":\n",
    "        t1w = sorted(glob.glob(f\"{PATH}/train/{scan_id}/T1w/*.png\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        img = get_slices(t1w, aug)\n",
    "    elif mri_type == \"T1wCE\":\n",
    "        t1wce = sorted(glob.glob(f\"{PATH}/train/{scan_id}/T1wCE/*.png\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        img = get_slices(t1wce, aug)\n",
    "    else:\n",
    "        t2w = sorted(glob.glob(f\"{PATH}/train/{scan_id}/T2w/*.png\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        img = get_slices(t2w, aug)\n",
    "    \n",
    "    # Return 3D image: ChannelsxWidthxHeightxDepth\n",
    "    img = img.reshape((1,config.RRC_SIZE,config.RRC_SIZE,config.SLICE_NUMBER))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a29f7-6588-4f19-8bef-f246a3dace77",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "The first row represents images with augmentation and the second row images without augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76f9233-737c-4bb0-888a-35733ddea734",
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_id = \"00000\"\n",
    "slice_nb = 0\n",
    "\n",
    "figure, axes = plt.subplots(2,4, figsize=(15,15))\n",
    "\n",
    "flair_3d_image = load_images(scan_id, \"FLAIR\", aug=True)\n",
    "flair_3d_image_fs = flair_3d_image[0,:,:,slice_nb]\n",
    "t1w_3d_image = load_images(scan_id, \"T1w\", aug=True)\n",
    "t1w_3d_image_fs = t1w_3d_image[0,:,:,slice_nb]\n",
    "t1wce_3d_image = load_images(scan_id, \"T1wCE\", aug=True)\n",
    "t1wce_3d_image_fs = t1wce_3d_image[0,:,:,slice_nb]\n",
    "t2w_3d_image = load_images(scan_id, \"T2w\", aug=True)\n",
    "t2w_3d_image_fs = t2w_3d_image[0,:,:,slice_nb]\n",
    "\n",
    "im = axes[0,0].imshow(flair_3d_image_fs)\n",
    "im = axes[0,1].imshow(t1w_3d_image_fs)\n",
    "im = axes[0,2].imshow(t1wce_3d_image_fs)\n",
    "im = axes[0,3].imshow(t2w_3d_image_fs)\n",
    "\n",
    "for ax in axes[0]:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "flair_3d_image = load_images(scan_id, \"FLAIR\", aug=False)\n",
    "flair_3d_image_fs = flair_3d_image[0,:,:,slice_nb]\n",
    "t1w_3d_image = load_images(scan_id, \"T1w\", aug=False)\n",
    "t1w_3d_image_fs = t1w_3d_image[0,:,:,slice_nb]\n",
    "t1wce_3d_image = load_images(scan_id, \"T1wCE\", aug=False)\n",
    "t1wce_3d_image_fs = t1wce_3d_image[0,:,:,slice_nb]\n",
    "t2w_3d_image = load_images(scan_id, \"T2w\", aug=False)\n",
    "t2w_3d_image_fs = t2w_3d_image[0,:,:,slice_nb]\n",
    "\n",
    "im = axes[1,0].imshow(flair_3d_image_fs)\n",
    "im = axes[1,1].imshow(t1w_3d_image_fs)\n",
    "im = axes[1,2].imshow(t1wce_3d_image_fs)\n",
    "im = axes[1,3].imshow(t2w_3d_image_fs)\n",
    "\n",
    "for ax in axes[1]:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e375fe33-206e-47fa-87ee-39e5c68ac346",
   "metadata": {},
   "source": [
    "### 1.2 Dataset and Dataloader\n",
    "\n",
    "Create a PyTorch Dataset and DataLoader **for each** MRI type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf7314b-c6de-4490-b3cc-ca6a5295c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNADataset(Dataset):\n",
    "    def __init__(self, path='../', split = \"train\", mri_type = \"FLAIR\", validation_split = 0.2):\n",
    "        train_data = pd.read_csv(os.path.join(path, 'train_labels.csv'))\n",
    "        self.labels = {}\n",
    "        self.mri_type = mri_type\n",
    "        brats = list(train_data[\"BraTS21ID\"])\n",
    "        mgmt = list(train_data[\"MGMT_value\"])\n",
    "        for b, m in zip(brats, mgmt):\n",
    "            self.labels[str(b).zfill(5)] = m\n",
    "            \n",
    "        remove_ids = [\"00709\", \"00109\", \"00123\"]\n",
    "            \n",
    "        if split == \"valid\":\n",
    "            self.split = split\n",
    "            self.ids = [a.split(\"/\")[-1] for a in sorted(glob.glob((path + f\"/train/\" + \"/*\")), key=lambda f: int(re.sub('\\D', '', f)))]\n",
    "            self.ids = self.ids[:int(len(self.ids) * validation_split)] # first 20% as validation\n",
    "        elif split == \"train\":\n",
    "            self.split = split\n",
    "            self.ids = [a.split(\"/\")[-1] for a in sorted(glob.glob((path + f\"/train/\" + \"/*\")), key=lambda f: int(re.sub('\\D', '', f)))]\n",
    "            self.ids = self.ids[int(len(self.ids) * validation_split):] # last 80% as train\n",
    "        elif split == \"test\":\n",
    "            self.split = split\n",
    "            self.ids = [a.split(\"/\")[-1] for a in sorted(glob.glob((path + f\"/test/\" + \"/*\")), key=lambda f: int(re.sub('\\D', '', f)))]\n",
    "        else:\n",
    "            self.split = split\n",
    "            self.ids = [a.split(\"/\")[-1] for a in sorted(glob.glob((path + f\"/train/\" + \"/*\")), key=lambda f: int(re.sub('\\D', '', f)))]\n",
    "        \n",
    "        self.ids = [id_ for id_ in self.ids if id_ not in remove_ids]            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.split == \"train\":\n",
    "            imgs = load_images(self.ids[idx], aug=True, mri_type=self.mri_type)\n",
    "        else:\n",
    "            imgs = load_images(self.ids[idx], aug=False, mri_type=self.mri_type)\n",
    "\n",
    "        if self.split != \"test\":\n",
    "            label = self.labels[self.ids[idx]]\n",
    "            return torch.tensor(imgs, dtype = torch.float32), torch.tensor(label, dtype = torch.long)\n",
    "        else:\n",
    "            return torch.tensor(imgs, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ef6f8-9b1f-4f4e-8eb0-cdedf122f584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAIR DataLoader\n",
    "train_flair_ds = RSNADataset(mri_type=\"FLAIR\")\n",
    "train_flair_dl = DataLoader(train_flair_ds, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "valid_flair_ds = RSNADataset(mri_type=\"FLAIR\", split='valid')\n",
    "valid_flair_dl = DataLoader(valid_flair_ds, batch_size=config.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ece82-0cfb-43bb-b0dc-ecec94512cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1w DataLoader\n",
    "train_t1w_ds = RSNADataset(mri_type=\"T1w\")\n",
    "train_t1w_dl = DataLoader(train_t1w_ds, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "valid_t1w_ds = RSNADataset(mri_type=\"T1w\", split='valid')\n",
    "valid_t1w_dl = DataLoader(valid_t1w_ds, batch_size=config.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29428828-7d92-4b34-856d-2de2e5fc0d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1wCE DataLoader\n",
    "train_t1wce_ds = RSNADataset(mri_type=\"T1wCE\")\n",
    "train_t1wce_dl = DataLoader(train_t1wce_ds, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "valid_t1wce_ds = RSNADataset(mri_type=\"T1wCE\", split='valid')\n",
    "valid_t1wce_dl = DataLoader(valid_t1wce_ds, batch_size=config.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b8f0b7-2c22-4882-ab36-b70db704acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2w DataLoader\n",
    "train_t2w_ds = RSNADataset(mri_type=\"T2w\")\n",
    "train_t2w_dl = DataLoader(train_t2w_ds, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "valid_t2w_ds = RSNADataset(mri_type=\"T2w\", split='valid')\n",
    "valid_t2w_dl = DataLoader(valid_t2w_ds, batch_size=config.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc05117-0a72-4c3f-ae15-629c568e0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_flair_dl))\n",
    "print(f\"Shape of the batch {images.shape}\")\n",
    "print(f\"Batch size: {images.shape[0]}\")\n",
    "print(f\"Number of channels each image has: {images.shape[1]}\")\n",
    "print(f\"Size of each image is: {images.shape[2]}x{images.shape[3]}\")\n",
    "print(f\"Depth of each channel/sequence: {images.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd7879-f56f-4241-b841-9ee3e85d46c2",
   "metadata": {},
   "source": [
    "### 1.3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd327d83-3b1f-48b2-b7dc-2a8531124fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inplanes():\n",
    "    return [64, 128, 256, 512]\n",
    "\n",
    "\n",
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=3,\n",
    "                     stride=stride,\n",
    "                     padding=1,\n",
    "                     bias=False)\n",
    "\n",
    "\n",
    "def conv1x1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=1,\n",
    "                     stride=stride,\n",
    "                     bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv3x3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv1x1x1(in_planes, planes)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.conv2 = conv3x3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.conv3 = conv1x1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 block,\n",
    "                 layers,\n",
    "                 block_inplanes,\n",
    "                 n_input_channels=1,\n",
    "                 conv1_t_size=7,\n",
    "                 conv1_t_stride=1,\n",
    "                 no_max_pool=False,\n",
    "                 shortcut_type='B',\n",
    "                 widen_factor=1.0,\n",
    "                 n_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        block_inplanes = [int(x * widen_factor) for x in block_inplanes]\n",
    "\n",
    "        self.in_planes = block_inplanes[0]\n",
    "        self.no_max_pool = no_max_pool\n",
    "\n",
    "        self.conv1 = nn.Conv3d(n_input_channels,\n",
    "                               self.in_planes,\n",
    "                               kernel_size=(conv1_t_size, 7, 7),\n",
    "                               stride=(conv1_t_stride, 2, 2),\n",
    "                               padding=(conv1_t_size // 2, 3, 3),\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(self.in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, block_inplanes[0], layers[0],\n",
    "                                       shortcut_type)\n",
    "        self.layer2 = self._make_layer(block,\n",
    "                                       block_inplanes[1],\n",
    "                                       layers[1],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "        self.layer3 = self._make_layer(block,\n",
    "                                       block_inplanes[2],\n",
    "                                       layers[2],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "        self.layer4 = self._make_layer(block,\n",
    "                                       block_inplanes[3],\n",
    "                                       layers[3],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(block_inplanes[3] * block.expansion, n_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight,\n",
    "                                        mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _downsample_basic_block(self, x, planes, stride):\n",
    "        out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
    "        zero_pads = torch.zeros(out.size(0), planes - out.size(1), out.size(2),\n",
    "                                out.size(3), out.size(4))\n",
    "        if isinstance(out.data, torch.cuda.FloatTensor):\n",
    "            zero_pads = zero_pads.cuda()\n",
    "\n",
    "        out = torch.cat([out.data, zero_pads], dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            if shortcut_type == 'A':\n",
    "                downsample = partial(self._downsample_basic_block,\n",
    "                                     planes=planes * block.expansion,\n",
    "                                     stride=stride)\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    conv1x1x1(self.in_planes, planes * block.expansion, stride),\n",
    "                    nn.BatchNorm3d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(in_planes=self.in_planes,\n",
    "                  planes=planes,\n",
    "                  stride=stride,\n",
    "                  downsample=downsample))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        if not self.no_max_pool:\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df09a3-54f9-4f8b-a083-122a50d8f565",
   "metadata": {},
   "source": [
    "### 1.4 Training\n",
    "\n",
    "Create a model for each MRI type, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1fe347-8945-4bee-84f4-4314911a81bb",
   "metadata": {},
   "source": [
    "#### 1.4.0 Trainings Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baecc1e-f75a-45f8-99a8-2541fe728813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(arr):\n",
    "    return [[1, 0] if a_i == 0 else [0, 1] for a_i in arr]\n",
    "\n",
    "def training(net, n_epochs, optimizer, loss_function, scheduler, train_dl, valid_dl, mri_type):\n",
    "    wandb.watch(net, loss_function, log='all', log_freq=10)\n",
    "    # Store the losses for each epoch\n",
    "    loss_train_list = []\n",
    "    loss_valid_list = []\n",
    "\n",
    "    # Store the roc for each epoch\n",
    "    roc_train_list = []\n",
    "    roc_valid_list = []\n",
    "    \n",
    "    best_roc = 0.0\n",
    "\n",
    "    # Iterate over the dataset n_epochs times\n",
    "    for epoch in range(n_epochs):\n",
    "        net.train()  # net.train() will notify all your layers that you are in training mode\n",
    "\n",
    "        train_loss = 0  # Training loss in epoch\n",
    "        y_train_list = []\n",
    "        y_hat_train_list = []\n",
    "        roc_train = 0.0\n",
    "\n",
    "        # For each batch, pass the training examples, calculate loss and gradients and optimize the parameters\n",
    "        for xb, yb in tqdm(train_dl, desc=\"Training\"):\n",
    "            optimizer.zero_grad()  # zero_grad clears old gradients from the last step\n",
    "\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            y_hat = net(xb)  # Forward pass\n",
    "            loss = loss_function(y_hat, yb)  # Calculate Loss\n",
    "\n",
    "            loss.backward()  # Calculate the gradients (using backpropagation)\n",
    "            optimizer.step()  # # Optimize the parameters: opt.step() causes the optimizer to take a step based on the gradients of the parameters.\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            y_train_list.extend(yb.tolist())\n",
    "            y_hat_train_list.extend(y_hat.tolist())\n",
    "\n",
    "        roc_train += roc_auc_score(one_hot(y_train_list), y_hat_train_list)\n",
    "                \n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"train_roc\": roc_train})\n",
    "\n",
    "        valid_loss = 0  # Validation loss in epoch\n",
    "        y_valid_list = []\n",
    "        y_hat_valid_list = []\n",
    "        roc_valid = 0.0\n",
    "\n",
    "        net.eval()  # net.eval() will notify all your layers that you are in evaluation mode\n",
    "        with torch.no_grad():\n",
    "            # Perform a prediction on the validation set  \n",
    "            for xb_valid, yb_valid in tqdm(valid_dl, desc=\"Validation\"):\n",
    "                xb_valid = xb_valid.to(device)\n",
    "                yb_valid = yb_valid.to(device)\n",
    "\n",
    "                y_hat = net(xb_valid)  # Forward pass\n",
    "                loss = loss_function(y_hat, yb_valid)  # Calculate Loss\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "                y_valid_list.extend(yb_valid.tolist())\n",
    "                y_hat_valid_list.extend(y_hat.tolist())\n",
    "\n",
    "        roc_valid += roc_auc_score(one_hot(y_valid_list), y_hat_valid_list)\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "        wandb.log({\"epoch\": epoch, \"test_loss\": valid_loss, \"test_roc\": roc_valid})\n",
    "\n",
    "        loss_train_list.append(train_loss)\n",
    "        loss_valid_list.append(valid_loss)\n",
    "        roc_train_list.append(roc_train)\n",
    "        roc_valid_list.append(roc_valid)\n",
    "        \n",
    "        if roc_valid > best_roc:\n",
    "            best_roc = roc_valid\n",
    "            torch.save(net.state_dict(), f'../models/01-3D-4-BaseCNN-{mri_type}-roc-{round(best_roc, 2)}.pt')\n",
    "\n",
    "    return roc_train_list, roc_valid_list, loss_train_list, loss_valid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43f7858-7aa8-4bf4-9a7b-524f19d82b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Loss Function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "def utilities():\n",
    "    # Create model\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], get_inplanes())\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr = config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "    # Scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.25)\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da4b08c-8126-4f9b-be92-2bc8d200c56c",
   "metadata": {},
   "source": [
    "#### 1.4.1 FLAIR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749e8f68-d129-4924-a6a1-141bf929a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_model, optimizer, scheduler = utilities()\n",
    "\n",
    "roc_train, roc_valid, loss_train, loss_valid = training(flair_model, config.N_EPOCHS, optimizer, loss_function, scheduler, train_flair_dl,\n",
    "                                                        valid_flair_dl, mri_type=\"FLAIR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9976a392-be38-4cf1-8fd7-d70016bde02a",
   "metadata": {},
   "source": [
    "#### 1.4.2 T1w model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b41f67-77cd-4c65-9d16-ecff770b32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1w_model, optimizer, scheduler = utilities()\n",
    "\n",
    "roc_train, roc_valid, loss_train, loss_valid = training(t1w_model, config.N_EPOCHS, optimizer, loss_function, scheduler, train_t1w_dl,\n",
    "                                                        valid_t1w_dl, mri_type=\"T1w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e80e916-cea2-40f4-a8bf-3037ab55b274",
   "metadata": {},
   "source": [
    "#### 1.4.3 T1wCE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb96a1e5-42f6-42ad-81c8-73ff81e8ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1wce_model, optimizer, scheduler = utilities()\n",
    "\n",
    "roc_train, roc_valid, loss_train, loss_valid = training(t1wce_model, config.N_EPOCHS, optimizer, loss_function, scheduler, train_t1wce_dl,\n",
    "                                                        valid_t1wce_dl, mri_type=\"T1wCE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4261591-9231-4f64-a16d-e8c810d4d47e",
   "metadata": {},
   "source": [
    "#### 1.4.4 T2w model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67b7a51-7aed-4f88-967c-8bf8e7ae8696",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2w_model, optimizer, scheduler = utilities()\n",
    "\n",
    "roc_train, roc_valid, loss_train, loss_valid = training(t2w_model, config.N_EPOCHS, optimizer, loss_function, scheduler, train_t2w_dl,\n",
    "                                                        valid_t2w_dl, mri_type=\"T2w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3b43ae-ccfd-4dea-a572-d7c19c6636ee",
   "metadata": {},
   "source": [
    "### 1.5 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230fa7a5-2bcc-42cf-a41b-d541c9d2a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(net, valid_dl):\n",
    "    \n",
    "    ys_hats = None\n",
    "    \n",
    "    net.eval()  # net.eval() will notify all your layers that you are in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Perform a prediction on the validation set  \n",
    "        for xb_valid, yb_valid in valid_dl:\n",
    "            xb_valid = xb_valid.to(device)\n",
    "            yb_valid = yb_valid.to(device)\n",
    "\n",
    "            y_hat = net(xb_valid)  # Forward pass\n",
    "            y_hat = F.softmax(y_hat)\n",
    "            if ys_hats is None:\n",
    "                ys_hats = y_hat.cpu().detach().numpy()\n",
    "            else:\n",
    "                ys_hats = np.concatenate((ys_hats, y_hat.cpu().detach().numpy()), axis=0)\n",
    "\n",
    "    return ys_hats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d5f4df-0915-43f4-859a-36ce5cb8a5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_y_hat = evaluation(flair_model, valid_flair_dl)\n",
    "t1w_y_hat = evaluation(t1w_model, valid_t1w_dl)\n",
    "t1wce_y_hat = evaluation(t1wce_model, valid_t1wce_dl)\n",
    "t2w_y_hat = evaluation(t2w_model, valid_t2w_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6085e01-bec8-40fc-9446-148574271451",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../train_labels.csv')\n",
    "df = df.iloc[0:int(df.shape[0]*0.2),:]\n",
    "df = df.loc[(df.BraTS21ID != 123) & (df.BraTS21ID != 109)]\n",
    "df[\"FLAIR\"] = flair_y_hat[:,1].tolist()\n",
    "df[\"T1w\"] = t1w_y_hat[:,1].tolist()\n",
    "df[\"T1wCE\"] = t1wce_y_hat[:,1].tolist()\n",
    "df[\"T2w\"] = t2w_y_hat[:,1].tolist()\n",
    "df[\"y_hat\"] = (df[\"FLAIR\"] + df[\"T1w\"] + df[\"T1wCE\"] + df[\"T2w\"]) / 4\n",
    "print(roc_auc_score(df[\"MGMT_value\"], df[\"y_hat\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbeffcf-dc09-4a18-9a0f-526216840ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb1d73c-6206-4a29-a9ef-51bb79550f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4f297-3162-44e2-a3dc-9de25b3b560b",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [pipenv: Kaggle]",
   "language": "python",
   "name": "kaggle_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
