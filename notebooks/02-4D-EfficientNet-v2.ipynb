{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c4df0b-8f20-49bc-9e75-fcc5e3985edd",
   "metadata": {},
   "source": [
    "# 4D EfficientNet\n",
    "\n",
    "In this notebook we'll train a 4D EfficientNet model using 4D images\n",
    "\n",
    "**4D images**\n",
    "* each image has the following shape: Channel x Width x Height x Depth\n",
    "* *channel:* the channel represents a specific MRI type. Further, each image contains all four MRI types (i.e., channel=4)\n",
    "* *depth:* the depth represents the depth or the number of slices of each image. I tried various values >= 30. Caution: If for a given patient id, the number of images <= the number of slices, we will replace the missing depth slices with zero matrices\n",
    "* added some albumentation such as CLAHE, brightness, and CoarseDropout\n",
    "* removed black pixels removed black pixels (see [Zabir Al Nazi Nabil](https://www.kaggle.com/furcifer/torch-efficientnet3d-for-mri-no-train))\n",
    "\n",
    "**4D EfficientNet**\n",
    "* Base: [EfficientNet-PyTorch-3D](https://github.com/shijianjian/EfficientNet-PyTorch-3D)\n",
    "* Custom head: One Fully-Connected Layer\n",
    "\n",
    "**Slicing Strategy**\n",
    "* take each x-th slice from each 3D image, whereby x (e.g., 2,4,6) depends on the maximum number of slices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890c327-af46-4b0a-ae4f-66b0b1eb80e1",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7beefc9-1c77-469d-aa86-0852ce4d8528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "\n",
    "import cv2\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "from efficientnet_pytorch_3d import EfficientNet3D\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69032e1-e9c2-42b2-ad4b-41828b74a756",
   "metadata": {},
   "source": [
    "#### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57758aa8-de49-474a-b7b9-23fc743a24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddc7dbb-4714-42e3-b8d7-ecfcf2ad6a1f",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eb5c48-d181-4483-b859-551b7d7a6bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '..'\n",
    "\n",
    "config = dict(\n",
    "    # Pre-processing\n",
    "    SLICE_NUMBER = 64, # >= 30\n",
    "    REMOVE_BLACK_BOUNDARIES = True,\n",
    "    DICOM=False,\n",
    "    \n",
    "    # Albumentation\n",
    "    RRC_SIZE = 256,\n",
    "    RRC_MIN_SCALE = 0.85,\n",
    "    RRC_RATIO = (1., 1.),\n",
    "    CLAHE_CLIP_LIMIT = 2.0,\n",
    "    CLAHE_TILE_GRID_SIZE = (8, 8),\n",
    "    CLAHE_PROB = 0.50,\n",
    "    BRIGHTNESS_LIMIT = (-0.2,0.2),\n",
    "    BRIGHTNESS_PROB = 0.40,\n",
    "    HUE_SHIFT = (-15, 15),\n",
    "    SAT_SHIFT = (-15, 15),\n",
    "    VAL_SHIFT = (-15, 15),\n",
    "    HUE_PROB = 0.64,\n",
    "    COARSE_MAX_HOLES = 16,\n",
    "    COARSE_PROB = 0.7,\n",
    "    \n",
    "    # Training\n",
    "    N_EPOCHS = 15,\n",
    "    BATCH_SIZE = 16,\n",
    "    LEARNING_RATE = 0.01,\n",
    "    WEIGHT_DECAY = 0.02,\n",
    "    OPTIMIZER = \"SGD\",\n",
    "    MOMENTUM = 0.9,\n",
    "    SCHEDULER = \"ReduceLROnPlateau\",\n",
    "    \n",
    "    # Logging\n",
    "    VERBOSE = False,\n",
    "    MODELNAME = \"02-4D-EfficientNet-v2\"\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6ace1-a624-45dd-a40a-18dcd36d340b",
   "metadata": {},
   "source": [
    "#### wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a67c0-30f1-4072-88e8-d89c4f346ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "run = wandb.init(entity=\"uzk-wim\", project='rsna-miccai-4D', config=config)\n",
    "config = wandb.config\n",
    "wandb.run.name = f\"{config.MODELNAME}-{str(config.SLICE_NUMBER)}-{str(config.RRC_SIZE)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe263350-abc7-47d3-a470-a57a7060b7e1",
   "metadata": {},
   "source": [
    "### 1. Load Data\n",
    "\n",
    "For each person, we were given *four* different MRI types: \n",
    "* FLAIR\n",
    "* T1w\n",
    "* T1wCE, and \n",
    "* T2w. \n",
    "\n",
    "We will create 4D images which are composed of each MRI type 3D image (4 sequences) where each sequence is composed of *config.SLICE_NUMBER* (e.g. 50) slices (i.e., depth).<br>\n",
    "Shape:  *Channel x Width x Height x Depth*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb4664-2143-4c1b-9f81-7a58d16edd73",
   "metadata": {},
   "source": [
    "### 1.1 Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2fba5-e062-4ab1-8b9e-ca72e9b36732",
   "metadata": {},
   "source": [
    "#### 1.1.1 Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c02d3-d25b-4c3f-95e5-545894a0a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.RandomResizedCrop(\n",
    "        config.RRC_SIZE, config.RRC_SIZE,            \n",
    "        scale=(config.RRC_MIN_SCALE, 1.0),\n",
    "        ratio=config.RRC_RATIO,\n",
    "        p=1.0\n",
    "    ),\n",
    "    A.CLAHE(\n",
    "        clip_limit=config.CLAHE_CLIP_LIMIT,\n",
    "        tile_grid_size=config.CLAHE_TILE_GRID_SIZE,\n",
    "        p=config.CLAHE_PROB\n",
    "    ),\n",
    "    A.RandomBrightnessContrast(\n",
    "        brightness_limit=config.BRIGHTNESS_LIMIT,\n",
    "        p=config.BRIGHTNESS_PROB\n",
    "    ),\n",
    "    A.HueSaturationValue(\n",
    "        hue_shift_limit=config.HUE_SHIFT, \n",
    "        sat_shift_limit=config.SAT_SHIFT, \n",
    "        val_shift_limit=config.VAL_SHIFT, \n",
    "        p=config.HUE_PROB\n",
    "    ),\n",
    "    A.CoarseDropout(\n",
    "        max_holes=config.COARSE_MAX_HOLES,\n",
    "        p=config.COARSE_PROB),\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    A.RandomResizedCrop( \n",
    "        config.RRC_SIZE, config.RRC_SIZE,            \n",
    "        scale=(config.RRC_MIN_SCALE, 1.0),\n",
    "        ratio=config.RRC_RATIO,\n",
    "        p=1.0\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7aedeb-df7f-45a4-9646-97bde2c10c19",
   "metadata": {},
   "source": [
    "#### 1.1.2 Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec6f51-ff10-46d5-a325-e90acd1e00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicom_2_image(file, voi_lut=True, fix_monochrome=True):\n",
    "    dicom = pydicom.read_file(file)\n",
    "    # VOI LUT (if available by DICOM device) is used to\n",
    "    # transform raw DICOM data to \"human-friendly\" view\n",
    "    if voi_lut:\n",
    "        img = apply_voi_lut(dicom.pixel_array, dicom)\n",
    "    else:\n",
    "        img = dicom.pixel_array\n",
    "    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "        img = np.amax(img) - img\n",
    "    \n",
    "    img = img - np.min(img)\n",
    "    img = img / np.max(img)\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "def remove_black_boundaries(img):\n",
    "    (x, y) = np.where(img > 0)\n",
    "    if len(x) > 0 and len(y) > 0:\n",
    "        x_mn = np.min(x)\n",
    "        x_mx = np.max(x)\n",
    "        y_mn = np.min(y)\n",
    "        y_mx = np.max(y)\n",
    "        if (x_mx - x_mn) > 10 and (y_mx - y_mn) > 10:\n",
    "            img = img[:,np.min(y):np.max(y)]\n",
    "    return img\n",
    "\n",
    "def get_idxs(img_depth):\n",
    "    minimum_idx = 0\n",
    "    maximum_idx = img_depth\n",
    "    step = 1\n",
    "    if config.SLICE_NUMBER < img_depth:\n",
    "        mod = img_depth % config.SLICE_NUMBER\n",
    "        threshold = int(mod // 2)\n",
    "        minimum_idx = threshold\n",
    "        maximum_idx = img_depth - threshold\n",
    "        step = img_depth // config.SLICE_NUMBER\n",
    "    if config.VERBOSE:\n",
    "        print(f\"Minimum {minimum_idx}\")\n",
    "        print(f\"Maximum {maximum_idx}\")\n",
    "        print(f\"Step size: {step}\")\n",
    "    return minimum_idx, maximum_idx, step\n",
    "\n",
    "def get_3d_image(mri_type, aug, dicom):\n",
    "    img_depth = len(mri_type)\n",
    "    if config.VERBOSE:\n",
    "        print(f\"Length of folder: {img_depth}\")\n",
    "    minimum_idx, maximum_idx, step = get_idxs(img_depth)\n",
    "    # Create array which contains the images\n",
    "    mri_img = []\n",
    "    counter = 0\n",
    "    for i in range(minimum_idx, maximum_idx, step):\n",
    "        file = mri_type[i]\n",
    "        if dicom:\n",
    "            img = dicom_2_image(file)\n",
    "        else:\n",
    "            img = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
    "        # Remove black boundaries\n",
    "        if config.REMOVE_BLACK_BOUNDARIES:\n",
    "            img = remove_black_boundaries(img)\n",
    "        if aug:\n",
    "            transformed = train_transform(image=img)\n",
    "            img = transformed[\"image\"]\n",
    "        else:\n",
    "            transformed = valid_transform(image=img)\n",
    "            img = transformed[\"image\"]\n",
    "        mri_img.append(np.array(img))\n",
    "        counter += 1\n",
    "        if counter == config.SLICE_NUMBER:\n",
    "            break\n",
    "    mri_img = np.array(mri_img).T\n",
    "    # If less than SLICE_NUMBER slices, add SLICE_NUMBER - mri_img.shape[-1] images with only zero values\n",
    "    if mri_img.shape[-1] < config.SLICE_NUMBER:\n",
    "        if config.VERBOSE:\n",
    "            print(f\"Current slices: {mri_img.shape[-1]}\")\n",
    "        n_zero = config.SLICE_NUMBER - mri_img.shape[-1]\n",
    "        mri_img = np.concatenate((mri_img, np.zeros((config.RRC_SIZE, config.RRC_SIZE, n_zero))), axis = -1)\n",
    "    if config.VERBOSE:\n",
    "        print(f\"Shape of mri_img: {mri_img.shape}\")\n",
    "    return mri_img\n",
    "    \n",
    "\n",
    "def load_images(scan_id, aug=True, split=\"train\", dicom=False):\n",
    "    # Ascending sort\n",
    "    file_ext = \"png\"\n",
    "    if dicom:\n",
    "        file_ext = \"dcm\"\n",
    "    flair = sorted(glob.glob(f\"{PATH}/{split}/{scan_id}/FLAIR/*.{file_ext}\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    t1w = sorted(glob.glob(f\"{PATH}/{split}/{scan_id}/T1w/*.{file_ext}\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    t1wce = sorted(glob.glob(f\"{PATH}/{split}/{scan_id}/T1wCE/*.{file_ext}\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    t2w = sorted(glob.glob(f\"{PATH}/{split}/{scan_id}/T2w/*.{file_ext}\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    \n",
    "    if config.VERBOSE:\n",
    "        print(f\"Scan id {scan_id}\")\n",
    "    flair_img = get_3d_image(flair, aug, dicom)\n",
    "    t1w_img = get_3d_image(t1w, aug, dicom)\n",
    "    t1wce_img = get_3d_image(t1wce, aug, dicom)\n",
    "    t2w_img = get_3d_image(t2w, aug, dicom)\n",
    "    \n",
    "    # Return 4D image: ChannelsxWidthxHeightxDepth\n",
    "    return np.moveaxis(np.array((flair_img, t1w_img, t1wce_img, t2w_img)), 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a29f7-6588-4f19-8bef-f246a3dace77",
   "metadata": {},
   "source": [
    "#### 1.1.3 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac668a6d-0146-4cc9-a7be-3f865e92f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2,4, figsize=(15,15))\n",
    "\n",
    "slices_aug_1 = load_images(\"00002\", aug=True)[0,:,:,0]\n",
    "slices_no_aug_1 = load_images(\"00002\", aug=False)[0,:,:,0]\n",
    "\n",
    "slices_aug_2 = load_images(\"00011\", aug=True)[0,:,:,0]\n",
    "slices_no_aug_2 = load_images(\"00011\", aug=False)[0,:,:,0]\n",
    "\n",
    "slices_aug_3 = load_images(\"00028\", aug=True)[0,:,:,0]\n",
    "slices_no_aug_3 = load_images(\"00028\", aug=False)[0,:,:,0]\n",
    "\n",
    "slices_aug_4 = load_images(\"00054\", aug=True)[0,:,:,0]\n",
    "slices_no_aug_4 = load_images(\"00054\", aug=False)[0,:,:,0]\n",
    "\n",
    "im = axes[0,0].imshow(slices_aug_1)\n",
    "im = axes[0,1].imshow(slices_aug_2)\n",
    "im = axes[0,2].imshow(slices_aug_3)\n",
    "im = axes[0,3].imshow(slices_aug_4)\n",
    "\n",
    "for ax in axes[0]:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "im = axes[1,0].imshow(slices_no_aug_1)\n",
    "im = axes[1,1].imshow(slices_no_aug_2)\n",
    "im = axes[1,2].imshow(slices_no_aug_3)\n",
    "im = axes[1,3].imshow(slices_no_aug_4)\n",
    "\n",
    "for ax in axes[1]:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e375fe33-206e-47fa-87ee-39e5c68ac346",
   "metadata": {},
   "source": [
    "### 1.2 Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb7ff2-5003-494b-8351-a3a4ba179ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ids and labels and make a stratified 80:20 split\n",
    "df = pd.read_csv(\"../train_labels.csv\")\n",
    "df_train, df_valid = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"MGMT_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcc75ba-1439-434e-b620-f6f8b4ca3cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNADataset(Dataset):\n",
    "    def __init__(self, ids, labels, split=\"train\", dicom=False):\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "        self.split = split\n",
    "        self.dicom = dicom\n",
    "        \n",
    "        if split == \"train\":\n",
    "            remove_ids = [709, 109, 123]\n",
    "            self.ids = [id_ for id_ in self.ids if id_ not in remove_ids]  \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.ids[idx]\n",
    "        patient_id = str(patient_id).zfill(5)\n",
    "        if self.split == \"train\":\n",
    "            imgs = load_images(patient_id, aug=True, dicom=self.dicom)\n",
    "        elif self.split == \"valid\":\n",
    "            imgs = load_images(patient_id, aug=False, dicom=self.dicom)\n",
    "        else:\n",
    "            imgs = load_images(patient_id, aug=False, split=self.split, dicom=self.dicom)\n",
    "        # Normalize\n",
    "        imgs = imgs - imgs.min()\n",
    "        imgs = (imgs + 1e-5) / (imgs.max() - imgs.min() + 1e-5)\n",
    "\n",
    "        if self.split != \"test\":\n",
    "            return torch.tensor(imgs, dtype = torch.float32), torch.tensor(self.labels[idx], dtype = torch.long)\n",
    "        else:\n",
    "            return torch.tensor(imgs, dtype = torch.float32), torch.tensor(self.ids[idx], dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01863a9-ba0e-46a2-9704-3428c3d77283",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = RSNADataset(df_train[\"BraTS21ID\"].to_numpy(), df_train[\"MGMT_value\"].to_numpy(), dicom=config.DICOM)\n",
    "train_dl = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "valid_ds = RSNADataset(df_valid[\"BraTS21ID\"].to_numpy(), df_valid[\"MGMT_value\"].to_numpy(), split='valid', dicom=config.DICOM)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=config.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc05117-0a72-4c3f-ae15-629c568e0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_dl))\n",
    "print(f\"Shape of the batch {images.shape}\")\n",
    "print(f\"Batch size: {images.shape[0]}\")\n",
    "print(f\"Number of channels each image has: {images.shape[1]}\")\n",
    "print(f\"Size of each image is: {images.shape[2]}x{images.shape[3]}\")\n",
    "print(f\"Depth of each channel/sequence: {images.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd7879-f56f-4241-b841-9ee3e85d46c2",
   "metadata": {},
   "source": [
    "### 1.3 Model\n",
    "\n",
    "#### Custom GliobCNN with 3D EfficientNet as base\n",
    "\n",
    "* Base: [EfficientNet-PyTorch-3D](https://github.com/shijianjian/EfficientNet-PyTorch-3D)\n",
    "* Custom head: One Fully-Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecd4db2-c103-400b-b777-e3ca76de4da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GliobCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GliobCNN, self).__init__()\n",
    "        \n",
    "        self.efficient = EfficientNet3D.from_name(\"efficientnet-b0\", override_params={'num_classes': 2}, in_channels=4)\n",
    "        self.efficient._fc = nn.Linear(in_features=self.efficient._fc.in_features, out_features=2, bias=True)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.efficient(X)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f772b43-a80c-4e6c-9487-d3ef304331b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GliobCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df09a3-54f9-4f8b-a083-122a50d8f565",
   "metadata": {},
   "source": [
    "### 1.4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baecc1e-f75a-45f8-99a8-2541fe728813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(net, n_epochs, optimizer, loss_function, scheduler):\n",
    "    wandb.watch(net, loss_function, log='all', log_freq=10)\n",
    "    \n",
    "    best_loss = np.inf\n",
    "\n",
    "    # Iterate over the dataset n_epochs times\n",
    "    for epoch in range(n_epochs):\n",
    "        net.train()  # net.train() will notify all your layers that you are in training mode\n",
    "\n",
    "        train_loss = 0  # Training loss in epoch\n",
    "        y_train_list = []\n",
    "        y_hat_train_list = []\n",
    "        roc_train = 0.0\n",
    "\n",
    "        # For each batch, pass the training examples, calculate loss and gradients and optimize the parameters\n",
    "        for xb, yb in tqdm(train_dl, desc=\"Training\"):\n",
    "            optimizer.zero_grad()  # zero_grad clears old gradients from the last step\n",
    "\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            y_hat = net(xb)  # Forward pass\n",
    "            loss = loss_function(y_hat, yb)  # Calculate Loss\n",
    "\n",
    "            loss.backward()  # Calculate the gradients (using backpropagation)\n",
    "            optimizer.step()  # # Optimize the parameters: opt.step() causes the optimizer to take a step based on the gradients of the parameters.\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            y_train_list.extend(yb.tolist())\n",
    "            y_hat = F.softmax(y_hat)\n",
    "            y_hat_train_list.extend(y_hat[:,1].tolist())\n",
    "            \n",
    "            if config.SCHEDULER == \"OneCycleLR\":\n",
    "                scheduler.step()\n",
    "                \n",
    "        y_hat_train_list = [1 if x > 0.5 else 0 for x in y_hat_train_list]\n",
    "        roc_train = roc_auc_score(y_train_list, y_hat_train_list)\n",
    "        \n",
    "        train_loss = train_loss/len(train_dl)\n",
    "                \n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"train_roc\": roc_train})\n",
    "\n",
    "        valid_loss = 0  # Validation loss in epoch\n",
    "        y_valid_list = []\n",
    "        y_hat_valid_list = []\n",
    "        roc_valid = 0.0\n",
    "\n",
    "        net.eval()  # net.eval() will notify all your layers that you are in evaluation mode\n",
    "        with torch.no_grad():\n",
    "            # Perform a prediction on the validation set  \n",
    "            for xb_valid, yb_valid in tqdm(valid_dl, desc=\"Validation\"):\n",
    "                xb_valid = xb_valid.to(device)\n",
    "                yb_valid = yb_valid.to(device)\n",
    "\n",
    "                y_hat = net(xb_valid)  # Forward pass\n",
    "                loss = loss_function(y_hat, yb_valid)  # Calculate Loss\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "                y_valid_list.extend(yb_valid.tolist())\n",
    "                y_hat = F.softmax(y_hat)\n",
    "                y_hat_valid_list.extend(y_hat[:,1].tolist())\n",
    "\n",
    "        y_hat_valid_list = [1 if x > 0.5 else 0 for x in y_hat_valid_list]\n",
    "        roc_valid = roc_auc_score(y_valid_list, y_hat_valid_list)\n",
    "        \n",
    "        valid_loss = valid_loss/len(valid_dl)\n",
    "        \n",
    "        if config.SCHEDULER == \"ReduceLROnPlateau\":\n",
    "            scheduler.step(valid_loss)\n",
    "\n",
    "        wandb.log({\"epoch\": epoch, \"valid_loss\": valid_loss, \"valid_roc\": roc_valid})\n",
    "        \n",
    "        if best_loss > valid_loss:\n",
    "            best_loss = valid_loss\n",
    "            wandb.run.summary[\"best_loss\"] = best_loss\n",
    "            wandb.run.summary[\"best_roc\"] = roc_valid\n",
    "            torch.save(net.state_dict(), f'../models/{config.MODELNAME}-loss-{round(valid_loss, 3)}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749e8f68-d129-4924-a6a1-141bf929a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Loss Function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create optimizer\n",
    "if config.OPTIMIZER == \"SGD\":\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config.LEARNING_RATE, momentum=config.MOMENTUM)\n",
    "elif config.OPTIMIZER == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr = config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr = config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "# Scheduler\n",
    "#  The 1cycle policy by fasti: https://sgugger.github.io/the-1cycle-policy.html\n",
    "if config.SCHEDULER == \"OneCycleLR\":\n",
    "    scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.03, steps_per_epoch=steps_per_epoch, epochs=config.N_EPOCHS)\n",
    "else:\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.25)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "training(model, config.N_EPOCHS, optimizer, loss_function, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb1d73c-6206-4a29-a9ef-51bb79550f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4f297-3162-44e2-a3dc-9de25b3b560b",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [pipenv: Kaggle]",
   "language": "python",
   "name": "kaggle_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
