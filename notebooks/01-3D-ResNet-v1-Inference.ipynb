{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c4df0b-8f20-49bc-9e75-fcc5e3985edd",
   "metadata": {},
   "source": [
    "# ResNet18\n",
    "\n",
    "In this notebook we'll train a 3D ResNet18 model using 3D images\n",
    "\n",
    "**3D images**\n",
    "* each image has the following shape: Channel x Width x Height x Depth\n",
    "* *channel:* the channel represents a specific MRI type. Further, each image contains all four MRI types (i.e., channel=4)\n",
    "* *depth:* the depth represents the depth or the number of slices. I tried various values >= 30. Caution: If for a given patient id, the number of images <= the number of slices, we will replace the missing depth slices with zero matrices\n",
    "* added some albumentation such as CLAHE, brightness, and CoarseDropout\n",
    "* removed black pixels removed black pixels (see [Zabir Al Nazi Nabil](https://www.kaggle.com/furcifer/torch-efficientnet3d-for-mri-no-train))\n",
    "\n",
    "**3D ResNet18**\n",
    "\n",
    "* copied from [3D-ResNets-PyTorch](https://github.com/kenshohara/3D-ResNets-PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890c327-af46-4b0a-ae4f-66b0b1eb80e1",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7beefc9-1c77-469d-aa86-0852ce4d8528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "\n",
    "import cv2\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69032e1-e9c2-42b2-ad4b-41828b74a756",
   "metadata": {},
   "source": [
    "#### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57758aa8-de49-474a-b7b9-23fc743a24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce6206b-ef78-4ce0-ac90-3cb20a1945dd",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ded9e2-4855-4ca2-bca3-1b5f0263ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../input/rsna-miccai-brain-tumor-radiogenomic-classification'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddc7dbb-4714-42e3-b8d7-ecfcf2ad6a1f",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e95b81-d9aa-489d-94c2-f2641cbd6121",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    # Pre-processing\n",
    "    SLICE_NUMBER = 50, # >= 30\n",
    "    REMOVE_BLACK_BOUNDARIES = True,\n",
    "    DICOM=False,\n",
    "    # Albumentation\n",
    "    RRC_SIZE = 256,\n",
    "    RRC_MIN_SCALE = 0.85,\n",
    "    RRC_RATIO = (1., 1.),\n",
    "    CLAHE_CLIP_LIMIT = 2.0,\n",
    "    CLAHE_TILE_GRID_SIZE = (8, 8),\n",
    "    CLAHE_PROB = 0.50,\n",
    "    BRIGHTNESS_LIMIT = (-0.2,0.2),\n",
    "    BRIGHTNESS_PROB = 0.40,\n",
    "    HUE_SHIFT = (-15, 15),\n",
    "    SAT_SHIFT = (-15, 15),\n",
    "    VAL_SHIFT = (-15, 15),\n",
    "    HUE_PROB = 0.64,\n",
    "    COARSE_MAX_HOLES = 16,\n",
    "    COARSE_PROB = 0.7,\n",
    "    # Training\n",
    "    N_EPOCHS = 20,\n",
    "    BATCH_SIZE = 4,\n",
    "    LEARNING_RATE = 0.001,\n",
    "    WEIGHT_DECAY = 0.02,\n",
    "    # Logging\n",
    "    VERBOSE = False,\n",
    "    MODELNAME = \"02-3D-ResNet101\"\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6ace1-a624-45dd-a40a-18dcd36d340b",
   "metadata": {},
   "source": [
    "#### wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b713f34-8251-4baf-810f-ce168323237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = config[\"MODELNAME\"]\n",
    "wandb.login()\n",
    "run = wandb.init(project='rsna-miccai', config=config, mode=\"disabled\")\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe263350-abc7-47d3-a470-a57a7060b7e1",
   "metadata": {},
   "source": [
    "### 1. Load Data\n",
    "\n",
    "For each person, we were given *four* different MRI types: \n",
    "* Fluid Attenuated Inversion Recovery (FLAIR)\n",
    "* T1-weighted pre-contrast (T1w)\n",
    "* T1-weighted post-contrast (T1Gd)\n",
    "* T2-weighted (T2)\n",
    "\n",
    "We will create 3D images which are composed of each MRI type (4 sequences) where each sequence is composed of *slice_number* (e.g. 50) slices (i.e., depth).<br>\n",
    "Shape:  *Channel x Width x Height x Depth*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb4664-2143-4c1b-9f81-7a58d16edd73",
   "metadata": {},
   "source": [
    "### 1.1 Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2fba5-e062-4ab1-8b9e-ca72e9b36732",
   "metadata": {},
   "source": [
    "#### 1.1.1 Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c02d3-d25b-4c3f-95e5-545894a0a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.RandomResizedCrop(\n",
    "        config.RRC_SIZE, config.RRC_SIZE,            \n",
    "        scale=(config.RRC_MIN_SCALE, 1.0),\n",
    "        ratio=config.RRC_RATIO,\n",
    "        p=1.0\n",
    "    ),\n",
    "    A.CLAHE(\n",
    "        clip_limit=config.CLAHE_CLIP_LIMIT,\n",
    "        tile_grid_size=config.CLAHE_TILE_GRID_SIZE,\n",
    "        p=config.CLAHE_PROB\n",
    "    ),\n",
    "    A.RandomBrightnessContrast(\n",
    "        brightness_limit=config.BRIGHTNESS_LIMIT,\n",
    "        p=config.BRIGHTNESS_PROB\n",
    "    ),\n",
    "    A.HueSaturationValue(\n",
    "        hue_shift_limit=config.HUE_SHIFT, \n",
    "        sat_shift_limit=config.SAT_SHIFT, \n",
    "        val_shift_limit=config.VAL_SHIFT, \n",
    "        p=config.HUE_PROB\n",
    "    ),\n",
    "    A.CoarseDropout(\n",
    "        max_holes=config.COARSE_MAX_HOLES,\n",
    "        p=config.COARSE_PROB),\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    A.RandomResizedCrop( \n",
    "        config.RRC_SIZE, config.RRC_SIZE,            \n",
    "        scale=(config.RRC_MIN_SCALE, 1.0),\n",
    "        ratio=config.RRC_RATIO,\n",
    "        p=1.0\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7aedeb-df7f-45a4-9646-97bde2c10c19",
   "metadata": {},
   "source": [
    "#### 1.1.2 Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec6f51-ff10-46d5-a325-e90acd1e00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicom_2_image(file, voi_lut=True, fix_monochrome=True):\n",
    "    dicom = pydicom.read_file(file)\n",
    "    # VOI LUT (if available by DICOM device) is used to\n",
    "    # transform raw DICOM data to \"human-friendly\" view\n",
    "    if voi_lut:\n",
    "        img = apply_voi_lut(dicom.pixel_array, dicom)\n",
    "    else:\n",
    "        img = dicom.pixel_array\n",
    "    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "        img = np.amax(img) - img\n",
    "    \n",
    "    img = img - np.min(img)\n",
    "    img = img / np.max(img)\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "def get_3d_image(mri_type, aug, dicom):\n",
    "    if config.VERBOSE:\n",
    "        print(f\"Length of folder: {len(mri_type)}\")\n",
    "    # Take SLICE_NUMBER slices from the middle\n",
    "    threshold = config.SLICE_NUMBER // 2\n",
    "    minimum_idx = len(mri_type)//2 - threshold if (len(mri_type)//2 - threshold) > 0 else 0\n",
    "    maximum_idx = len(mri_type)//2 + threshold  # maximum can exceed the index\n",
    "    if config.VERBOSE:\n",
    "        print(f\"Minimum {minimum_idx}\")\n",
    "        print(f\"Maximum {maximum_idx}\")\n",
    "    # Create array which contains the images\n",
    "    mri_img = []\n",
    "    for file in mri_type[minimum_idx:maximum_idx]:\n",
    "        if dicom:\n",
    "            img = dicom_2_image(file)\n",
    "        else:\n",
    "            img = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
    "        if config.REMOVE_BLACK_BOUNDARIES:\n",
    "            (x, y) = np.where(img > 0)\n",
    "            if len(x) > 0 and len(y) > 0:\n",
    "                x_mn = np.min(x)\n",
    "                x_mx = np.max(x)\n",
    "                y_mn = np.min(y)\n",
    "                y_mx = np.max(y)\n",
    "                if (x_mx - x_mn) > 10 and (y_mx - y_mn) > 10:\n",
    "                    img = img[:,np.min(y):np.max(y)]\n",
    "        if aug:\n",
    "            transformed = train_transform(image=img)\n",
    "            img = transformed[\"image\"]\n",
    "        else:\n",
    "            transformed = valid_transform(image=img)\n",
    "            img = transformed[\"image\"]\n",
    "        mri_img.append(np.array(img))\n",
    "    mri_img = np.array(mri_img).T\n",
    "    # If less than SLICE_NUMBER slices, add SLICE_NUMBER - mri_img.shape[-1] images with only zero values\n",
    "    if mri_img.shape[-1] < config.SLICE_NUMBER:\n",
    "        if config.VERBOSE:\n",
    "            print(f\"Current slices: {mri_img.shape[-1]}\")\n",
    "        n_zero = config.SLICE_NUMBER - mri_img.shape[-1]\n",
    "        mri_img = np.concatenate((mri_img, np.zeros((config.RRC_SIZE, config.RRC_SIZE, n_zero))), axis = -1)\n",
    "    if config.VERBOSE:\n",
    "        print(f\"Shape of mri_img: {mri_img.shape}\")\n",
    "    return mri_img\n",
    "    \n",
    "\n",
    "def load_images(scan_id, aug=True, split=\"train\", dicom=False):\n",
    "    # Ascending sort\n",
    "    file_ext = \"png\"\n",
    "    if dicom:\n",
    "        file_ext = \"dcm\"\n",
    "    flair = sorted(glob.glob(f\"{PATH}/{split}/{scan_id}/FLAIR/*.{file_ext}\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    t1w = sorted(glob.glob(f\"{PATH}/{split}/{scan_id}/T1w/*.{file_ext}\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    t1wce = sorted(glob.glob(f\"{PATH}/{split}/{scan_id}/T1wCE/*.{file_ext}\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    t2w = sorted(glob.glob(f\"{PATH}/{split}/{scan_id}/T2w/*.{file_ext}\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    \n",
    "    if config.VERBOSE:\n",
    "        print(f\"Scan id {scan_id}\")\n",
    "    flair_img = get_3d_image(flair, aug, dicom)\n",
    "    t1w_img = get_3d_image(t1w, aug, dicom)\n",
    "    t1wce_img = get_3d_image(t1wce, aug, dicom)\n",
    "    t2w_img = get_3d_image(t2w, aug, dicom)\n",
    "    \n",
    "    # Return 3D image: ChannelsxWidthxHeightxDepth\n",
    "    return np.moveaxis(np.array((flair_img, t1w_img, t1wce_img, t2w_img)), 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e375fe33-206e-47fa-87ee-39e5c68ac346",
   "metadata": {},
   "source": [
    "### 1.2 Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb7ff2-5003-494b-8351-a3a4ba179ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ids and labels and make a stratified 80:20 split\n",
    "df_test = pd.read_csv(f\"{PATH}/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8946b-17a0-4e2a-96f3-41937553e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNADataset(Dataset):\n",
    "    def __init__(self, ids, labels, split = \"train\", dicom=False):\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "        self.split = split\n",
    "        self.dicom = dicom\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.ids[idx]\n",
    "        patient_id = str(patient_id).zfill(5)\n",
    "        if self.split == \"train\":\n",
    "            imgs = load_images(patient_id, aug=True, dicom=self.dicom)\n",
    "        elif self.split == \"valid\":\n",
    "            imgs = load_images(patient_id, aug=False, dicom=self.dicom)\n",
    "        else:\n",
    "            imgs = load_images(patient_id, aug=False, split=self.split, dicom=self.dicom)\n",
    "        # Normalize\n",
    "        imgs = imgs - imgs.min()\n",
    "        imgs = (imgs + 1e-5) / (imgs.max() - imgs.min() + 1e-5)\n",
    "\n",
    "        if self.split != \"test\":\n",
    "            label = self.labels[idx]\n",
    "            return torch.tensor(imgs, dtype = torch.float32), torch.tensor(label, dtype = torch.long)\n",
    "        else:\n",
    "            label = self.labels[idx]\n",
    "            return torch.tensor(imgs, dtype = torch.float32), torch.tensor(self.ids[idx], dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01863a9-ba0e-46a2-9704-3428c3d77283",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = RSNADataset(df_test[\"BraTS21ID\"].to_numpy(), df_test[\"MGMT_value\"].to_numpy(), split='test', dicom=config.DICOM)\n",
    "test_dl = DataLoader(test_ds, batch_size=config[\"BATCH_SIZE\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc05117-0a72-4c3f-ae15-629c568e0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, idx = next(iter(test_dl))\n",
    "print(f\"Shape of the batch {images.shape}\")\n",
    "print(f\"Batch size: {images.shape[0]}\")\n",
    "print(f\"Number of channels each image has: {images.shape[1]}\")\n",
    "print(f\"Size of each image is: {images.shape[2]}x{images.shape[3]}\")\n",
    "print(f\"Depth of each channel/sequence: {images.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd7879-f56f-4241-b841-9ee3e85d46c2",
   "metadata": {},
   "source": [
    "### 1.3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dce196b-67eb-401d-a415-f8a56eb0f42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "def conv1x1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv3x3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv1x1x1(in_planes, planes)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.conv2 = conv3x3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.conv3 = conv1x1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, block_inplanes, n_input_channels=4, conv1_t_size=7, conv1_t_stride=1, no_max_pool=False,\n",
    "                 shortcut_type='B', widen_factor=1.0, n_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # For wider ResNets only\n",
    "        block_inplanes = [int(x * widen_factor) for x in block_inplanes]\n",
    "\n",
    "        self.in_planes = block_inplanes[0]  # 64\n",
    "        self.no_max_pool = no_max_pool\n",
    "\n",
    "        # First Block: Convolutional Layer + Batch Normalization + ReLu + Max Pooling\n",
    "        self.conv1 = nn.Conv3d(n_input_channels, self.in_planes, kernel_size=(conv1_t_size, 7, 7), stride=(conv1_t_stride, 2, 2),\n",
    "                               padding=(conv1_t_size // 2, 3, 3), bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(self.in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "        # Example ResNet18: Each layer consists of two residual blocks and two residual connections\n",
    "        self.layer1 = self._make_layer(block, block_inplanes[0], layers[0], shortcut_type)\n",
    "        self.layer2 = self._make_layer(block, block_inplanes[1], layers[1], shortcut_type, stride=2)\n",
    "        self.layer3 = self._make_layer(block, block_inplanes[2], layers[2], shortcut_type, stride=2)\n",
    "        self.layer4 = self._make_layer(block, block_inplanes[3], layers[3], shortcut_type, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(block_inplanes[3] * block.expansion, n_classes)\n",
    "\n",
    "        # Initialization\n",
    "        for m in self.modules():\n",
    "            # He initialization\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _downsample_basic_block(self, x, planes, stride):\n",
    "        out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
    "        zero_pads = torch.zeros(out.size(0), planes - out.size(1), out.size(2), out.size(3), out.size(4))\n",
    "        if isinstance(out.data, torch.cuda.FloatTensor):\n",
    "            zero_pads = zero_pads.cuda()\n",
    "\n",
    "        out = torch.cat([out.data, zero_pads], dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            if shortcut_type == 'A':\n",
    "                downsample = partial(self._downsample_basic_block, planes=planes * block.expansion, stride=stride)\n",
    "            else:\n",
    "                downsample = nn.Sequential(conv1x1x1(self.in_planes, planes * block.expansion, stride), \n",
    "                                           nn.BatchNorm3d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(in_planes=self.in_planes, planes=planes, stride=stride, downsample=downsample))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        if not self.no_max_pool:\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aff47e-0fc7-460a-bedc-29f72966b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet18\n",
    "#model = ResNet(BasicBlock, [2, 2, 2, 2], [64, 128, 256, 512])\n",
    "# ResNet34\n",
    "#model = ResNet(BasicBlock, [3, 4, 6, 3], [64, 128, 256, 512])\n",
    "# ResNet50\n",
    "model = ResNet(Bottleneck, [3, 4, 6, 3], [64, 128, 256, 512])\n",
    "# ResNet101\n",
    "#model = ResNet(Bottleneck, [3, 4, 23, 3], [64, 128, 256, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80dd68a-a9f9-4bb1-80da-ec8b6134d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/02-3D-ResNet50-roc-0.72.pt\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df09a3-54f9-4f8b-a083-122a50d8f565",
   "metadata": {},
   "source": [
    "### 1.4 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac24f83-34d2-4ff6-99cf-c785deb98069",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hats = None\n",
    "idx_list = []\n",
    "\n",
    "for xb, idxb in tqdm(test_dl, desc=\"Testing\"):\n",
    "    xb = xb.to(device)\n",
    "    y_hat = model(xb)\n",
    "    y_hat = F.softmax(y_hat)[:,1].cpu().detach().numpy()\n",
    "    if y_hats is None:\n",
    "        y_hats = y_hat\n",
    "        idx_list = idxb.numpy()\n",
    "    else:\n",
    "        y_hats = np.concatenate((y_hats, y_hat), axis=0)\n",
    "        idx_list = np.concatenate((idx_list, idxb.numpy()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbbb940-6e1a-4481-b957-7c50f03eb66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame({'BraTS21ID': idx_list, 'MGMT_value': y_hats})\n",
    "df_submission.to_csv(\"submission.csv\", index=False)\n",
    "df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb1d73c-6206-4a29-a9ef-51bb79550f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4f297-3162-44e2-a3dc-9de25b3b560b",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [pipenv: Kaggle]",
   "language": "python",
   "name": "kaggle_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
