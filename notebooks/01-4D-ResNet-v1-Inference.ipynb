{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c4df0b-8f20-49bc-9e75-fcc5e3985edd",
   "metadata": {},
   "source": [
    "# 4D ResNet18 Inference\n",
    "\n",
    "## Kaggle Version\n",
    "\n",
    "In this notebook we'll use a \"4D\" ResNet model (strickly speaking: it's still a 3D ResNet) using \"4D\" images.<br>\n",
    "We will use 5-fold CV to make the outcome more reliable.\n",
    "\n",
    "**4D images**\n",
    "* each image has the following shape: Channel x Width x Height x Depth\n",
    "* *channel:* the channel represents a specific MRI type. Further, <u>each</u> image contains all four MRI types (i.e., channel=4) --> That is why we say it is \"4D\"\n",
    "* *depth:* the depth represents the depth or the number of slices. I tried various values >= 30. Caution: If for a given patient id, the number of images <= the number of slices, we will replace the missing depth slices with zero matrices.\n",
    "* added some albumentation such as CLAHE, brightness, and CoarseDropout\n",
    "* removed black pixels (see [Zabir Al Nazi Nabil](https://www.kaggle.com/furcifer/torch-efficientnet3d-for-mri-no-train))\n",
    "\n",
    "**4D ResNet18**\n",
    "* copied and edited from [3D-ResNets-PyTorch](https://github.com/kenshohara/3D-ResNets-PyTorch)\n",
    "\n",
    "**Slicing Strategy**\n",
    "* take SLICE_NUMBER slices from the middle of each 3D image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890c327-af46-4b0a-ae4f-66b0b1eb80e1",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7beefc9-1c77-469d-aa86-0852ce4d8528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "\n",
    "import cv2\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69032e1-e9c2-42b2-ad4b-41828b74a756",
   "metadata": {},
   "source": [
    "#### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57758aa8-de49-474a-b7b9-23fc743a24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce6206b-ef78-4ce0-ac90-3cb20a1945dd",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ded9e2-4855-4ca2-bca3-1b5f0263ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../input/rsna-miccai-brain-tumor-radiogenomic-classification'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddc7dbb-4714-42e3-b8d7-ecfcf2ad6a1f",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e95b81-d9aa-489d-94c2-f2641cbd6121",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = dict(\n",
    "    # Pre-processing\n",
    "    SLICE_NUMBER = 32, # >= 30\n",
    "    REMOVE_BLACK_BOUNDARIES = True,\n",
    "    DICOM=True,\n",
    "    \n",
    "    # Albumentation\n",
    "    RRC_SIZE = 256,\n",
    "    RRC_MIN_SCALE = 0.85,\n",
    "    RRC_RATIO = (1., 1.),\n",
    "    CLAHE_CLIP_LIMIT = 2.0,\n",
    "    CLAHE_TILE_GRID_SIZE = (8, 8),\n",
    "    CLAHE_PROB = 0.50,\n",
    "    BRIGHTNESS_LIMIT = (-0.2,0.2),\n",
    "    BRIGHTNESS_PROB = 0.40,\n",
    "    HUE_SHIFT = (-15, 15),\n",
    "    SAT_SHIFT = (-15, 15),\n",
    "    VAL_SHIFT = (-15, 15),\n",
    "    HUE_PROB = 0.64,\n",
    "    COARSE_MAX_HOLES = 16,\n",
    "    COARSE_PROB = 0.7,\n",
    "    \n",
    "    # K-Fold\n",
    "    N_SPLITS = 5,\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 16,\n",
    "    \n",
    "    # Logging\n",
    "    VERBOSE = False,\n",
    "    MODELNAME = \"01-4D-ResNet-v1\"\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6ace1-a624-45dd-a40a-18dcd36d340b",
   "metadata": {},
   "source": [
    "#### wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b713f34-8251-4baf-810f-ce168323237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "run_name = f'{config_dict[\"MODELNAME\"]}-{str(config_dict[\"SLICE_NUMBER\"])}-{str(config_dict[\"RRC_SIZE\"])}'\n",
    "run = wandb.init(entity=\"uzk-wim\", project='rsna-miccai-k-fold', config=config_dict, mode=\"offline\", group=run_name)\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe263350-abc7-47d3-a470-a57a7060b7e1",
   "metadata": {},
   "source": [
    "### 1. Load Data\n",
    "\n",
    "For each person, we were given *four* different MRI types: \n",
    "* Fluid Attenuated Inversion Recovery (FLAIR)\n",
    "* T1-weighted pre-contrast (T1w)\n",
    "* T1-weighted post-contrast (T1Gd)\n",
    "* T2-weighted (T2)\n",
    "\n",
    "We will create 3D images which are composed of each MRI type (4 sequences) where each sequence is composed of *slice_number* (e.g. 50) slices (i.e., depth).<br>\n",
    "Shape:  *Channel x Width x Height x Depth*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb4664-2143-4c1b-9f81-7a58d16edd73",
   "metadata": {},
   "source": [
    "### 1.1 Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2fba5-e062-4ab1-8b9e-ca72e9b36732",
   "metadata": {},
   "source": [
    "#### 1.1.1 Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c02d3-d25b-4c3f-95e5-545894a0a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.RandomResizedCrop(\n",
    "        config.RRC_SIZE, config.RRC_SIZE,            \n",
    "        scale=(config.RRC_MIN_SCALE, 1.0),\n",
    "        ratio=config.RRC_RATIO,\n",
    "        p=1.0\n",
    "    ),\n",
    "    A.CLAHE(\n",
    "        clip_limit=config.CLAHE_CLIP_LIMIT,\n",
    "        tile_grid_size=config.CLAHE_TILE_GRID_SIZE,\n",
    "        p=config.CLAHE_PROB\n",
    "    ),\n",
    "    A.RandomBrightnessContrast(\n",
    "        brightness_limit=config.BRIGHTNESS_LIMIT,\n",
    "        p=config.BRIGHTNESS_PROB\n",
    "    ),\n",
    "    A.HueSaturationValue(\n",
    "        hue_shift_limit=config.HUE_SHIFT, \n",
    "        sat_shift_limit=config.SAT_SHIFT, \n",
    "        val_shift_limit=config.VAL_SHIFT, \n",
    "        p=config.HUE_PROB\n",
    "    ),\n",
    "    A.CoarseDropout(\n",
    "        max_holes=config.COARSE_MAX_HOLES,\n",
    "        p=config.COARSE_PROB),\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    A.RandomResizedCrop( \n",
    "        config.RRC_SIZE, config.RRC_SIZE,            \n",
    "        scale=(config.RRC_MIN_SCALE, 1.0),\n",
    "        ratio=config.RRC_RATIO,\n",
    "        p=1.0\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7aedeb-df7f-45a4-9646-97bde2c10c19",
   "metadata": {},
   "source": [
    "#### 1.1.2 Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec6f51-ff10-46d5-a325-e90acd1e00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicom_2_image(file, voi_lut=True, fix_monochrome=True):\n",
    "    dicom = pydicom.read_file(file)\n",
    "    # VOI LUT (if available by DICOM device) is used to\n",
    "    # transform raw DICOM data to \"human-friendly\" view\n",
    "    if voi_lut:\n",
    "        img = apply_voi_lut(dicom.pixel_array, dicom)\n",
    "    else:\n",
    "        img = dicom.pixel_array\n",
    "    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "        img = np.amax(img) - img\n",
    "    \n",
    "    img = img - np.min(img)\n",
    "    img = img / np.max(img)\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "def remove_black_boundaries(img):\n",
    "    (x, y) = np.where(img > 0)\n",
    "    if len(x) > 0 and len(y) > 0:\n",
    "        x_mn = np.min(x)\n",
    "        x_mx = np.max(x)\n",
    "        y_mn = np.min(y)\n",
    "        y_mx = np.max(y)\n",
    "        if (x_mx - x_mn) > 10 and (y_mx - y_mn) > 10:\n",
    "            img = img[:,np.min(y):np.max(y)]\n",
    "    return img\n",
    "\n",
    "def get_middle_idxs(mri_type):\n",
    "    # Take SLICE_NUMBER slices from the middle\n",
    "    threshold = config.SLICE_NUMBER // 2\n",
    "    minimum_idx = len(mri_type)//2 - threshold if (len(mri_type)//2 - threshold) > 0 else 0\n",
    "    maximum_idx = len(mri_type)//2 + threshold  # maximum can exceed the index\n",
    "    step = 1\n",
    "    if config.VERBOSE:\n",
    "        print(f\"Minimum {minimum_idx}\")\n",
    "        print(f\"Maximum {maximum_idx}\")\n",
    "    return minimum_idx, maximum_idx, step\n",
    "\n",
    "def get_3d_image(mri_type, aug, dicom):\n",
    "    minimum_idx, maximum_idx, step = get_middle_idxs(mri_type)\n",
    "    \n",
    "    # Create list which contains all the 2D images which form the 3D image\n",
    "    mri_img = []\n",
    "    for i in range(minimum_idx, maximum_idx, step):\n",
    "        if i >= len(mri_type):\n",
    "            break\n",
    "        file = mri_type[i]\n",
    "        \n",
    "        # Load 2D image\n",
    "        if dicom:\n",
    "            img = dicom_2_image(file)\n",
    "        else:\n",
    "            img = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "        # Remove black boundaries\n",
    "        if config.REMOVE_BLACK_BOUNDARIES:\n",
    "            img = remove_black_boundaries(img)\n",
    "            \n",
    "        # Augmentation\n",
    "        if aug:\n",
    "            transformed = train_transform(image=img)\n",
    "            img = transformed[\"image\"]\n",
    "        else:\n",
    "            transformed = valid_transform(image=img)\n",
    "            img = transformed[\"image\"]\n",
    "            \n",
    "        mri_img.append(np.array(img))\n",
    "    mri_img = np.array(mri_img).T\n",
    "    \n",
    "    # If less than SLICE_NUMBER slices, add SLICE_NUMBER - mri_img.shape[-1] images with only zero values\n",
    "    if mri_img.shape[-1] < config.SLICE_NUMBER:\n",
    "        if config.VERBOSE:\n",
    "            print(f\"Current slices: {mri_img.shape[-1]}\")\n",
    "        n_zero = config.SLICE_NUMBER - mri_img.shape[-1]\n",
    "        mri_img = np.concatenate((mri_img, np.zeros((config.RRC_SIZE, config.RRC_SIZE, n_zero))), axis = -1)\n",
    "    if config.VERBOSE:\n",
    "        print(f\"Shape of mri_img: {mri_img.shape}\")\n",
    "    return mri_img\n",
    "    \n",
    "def load_images(scan_id, aug=True, split=\"train\", dicom=False):\n",
    "    file_ext = \"png\"\n",
    "    if dicom:\n",
    "        file_ext = \"dcm\"\n",
    "    # Ascending sort\n",
    "    flair = sorted(glob.glob(f\"{PATH}/{split}/{scan_id}/FLAIR/*.{file_ext}\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    t1w = sorted(glob.glob(f\"{PATH}/{split}/{scan_id}/T1w/*.{file_ext}\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    t1wce = sorted(glob.glob(f\"{PATH}/{split}/{scan_id}/T1wCE/*.{file_ext}\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    t2w = sorted(glob.glob(f\"{PATH}/{split}/{scan_id}/T2w/*.{file_ext}\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "    \n",
    "    if config.VERBOSE:\n",
    "        print(f\"Scan id {scan_id}\")\n",
    "    flair_img = get_3d_image(flair, aug, dicom)\n",
    "    t1w_img = get_3d_image(t1w, aug, dicom)\n",
    "    t1wce_img = get_3d_image(t1wce, aug, dicom)\n",
    "    t2w_img = get_3d_image(t2w, aug, dicom)\n",
    "    \n",
    "    img = np.moveaxis(np.array((flair_img, t1w_img, t1wce_img, t2w_img)), 0, 0)\n",
    "    \n",
    "    # Normalization\n",
    "    img = img - img.min()\n",
    "    img = (img + 1e-5) / (img.max() - img.min() + 1e-5)\n",
    "    # Return 4D image: ChannelsxWidthxHeightxDepth\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e375fe33-206e-47fa-87ee-39e5c68ac346",
   "metadata": {},
   "source": [
    "### 1.2 Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb7ff2-5003-494b-8351-a3a4ba179ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ids and labels and make a stratified 80:20 split\n",
    "df_test = pd.read_csv(f\"{PATH}/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8946b-17a0-4e2a-96f3-41937553e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNADataset(Dataset):\n",
    "    def __init__(self, ids, labels, split = \"train\", dicom=False):\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "        self.split = split\n",
    "        self.dicom = dicom\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.ids[idx]\n",
    "        patient_id = str(patient_id).zfill(5)\n",
    "        if self.split == \"train\":\n",
    "            imgs = load_images(patient_id, aug=True, dicom=self.dicom)\n",
    "        elif self.split == \"valid\":\n",
    "            imgs = load_images(patient_id, aug=False, dicom=self.dicom)\n",
    "        else:\n",
    "            imgs = load_images(patient_id, aug=False, split=self.split, dicom=self.dicom)\n",
    "\n",
    "        if self.split != \"test\":\n",
    "            return torch.tensor(imgs, dtype = torch.float32), torch.tensor(self.labels[idx], dtype = torch.long)\n",
    "        else:\n",
    "            return torch.tensor(imgs, dtype = torch.float32), torch.tensor(self.ids[idx], dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01863a9-ba0e-46a2-9704-3428c3d77283",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = RSNADataset(df_test[\"BraTS21ID\"].to_numpy(), df_test[\"MGMT_value\"].to_numpy(), split='test', dicom=config.DICOM)\n",
    "test_dl = DataLoader(test_ds, batch_size=config[\"BATCH_SIZE\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc05117-0a72-4c3f-ae15-629c568e0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, idx = next(iter(test_dl))\n",
    "print(f\"Shape of the batch {images.shape}\")\n",
    "print(f\"Batch size: {images.shape[0]}\")\n",
    "print(f\"Number of channels each image has: {images.shape[1]}\")\n",
    "print(f\"Size of each image is: {images.shape[2]}x{images.shape[3]}\")\n",
    "print(f\"Depth of each channel/sequence: {images.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd7879-f56f-4241-b841-9ee3e85d46c2",
   "metadata": {},
   "source": [
    "### 1.3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dce196b-67eb-401d-a415-f8a56eb0f42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "def conv1x1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv3x3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv1x1x1(in_planes, planes)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.conv2 = conv3x3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.conv3 = conv1x1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, block_inplanes, n_input_channels=4, conv1_t_size=7, conv1_t_stride=1, no_max_pool=False,\n",
    "                 shortcut_type='B', widen_factor=1.0, n_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # For wider ResNets only\n",
    "        block_inplanes = [int(x * widen_factor) for x in block_inplanes]\n",
    "\n",
    "        self.in_planes = block_inplanes[0]  # 64\n",
    "        self.no_max_pool = no_max_pool\n",
    "\n",
    "        # First Block: Convolutional Layer + Batch Normalization + ReLu + Max Pooling\n",
    "        self.conv1 = nn.Conv3d(n_input_channels, self.in_planes, kernel_size=(conv1_t_size, 7, 7), stride=(conv1_t_stride, 2, 2),\n",
    "                               padding=(conv1_t_size // 2, 3, 3), bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(self.in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "        # Example ResNet18: Each layer consists of two residual blocks and two residual connections\n",
    "        self.layer1 = self._make_layer(block, block_inplanes[0], layers[0], shortcut_type)\n",
    "        self.layer2 = self._make_layer(block, block_inplanes[1], layers[1], shortcut_type, stride=2)\n",
    "        self.layer3 = self._make_layer(block, block_inplanes[2], layers[2], shortcut_type, stride=2)\n",
    "        self.layer4 = self._make_layer(block, block_inplanes[3], layers[3], shortcut_type, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(block_inplanes[3] * block.expansion, n_classes)\n",
    "\n",
    "        # Initialization\n",
    "        for m in self.modules():\n",
    "            # He initialization\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _downsample_basic_block(self, x, planes, stride):\n",
    "        out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
    "        zero_pads = torch.zeros(out.size(0), planes - out.size(1), out.size(2), out.size(3), out.size(4))\n",
    "        if isinstance(out.data, torch.cuda.FloatTensor):\n",
    "            zero_pads = zero_pads.cuda()\n",
    "\n",
    "        out = torch.cat([out.data, zero_pads], dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            if shortcut_type == 'A':\n",
    "                downsample = partial(self._downsample_basic_block, planes=planes * block.expansion, stride=stride)\n",
    "            else:\n",
    "                downsample = nn.Sequential(conv1x1x1(self.in_planes, planes * block.expansion, stride), \n",
    "                                           nn.BatchNorm3d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(in_planes=self.in_planes, planes=planes, stride=stride, downsample=downsample))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        if not self.no_max_pool:\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b389303-adac-448a-b051-8c143782e045",
   "metadata": {},
   "source": [
    "### 1.4 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f4e1f-c732-4b56-9018-38b247499e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0_path = \"01-4D-ResNet-v1-fold-0-loss-0.707.pt\"\n",
    "model_1_path = \"01-4D-ResNet-v1-fold-1-loss-0.662.pt\"\n",
    "model_2_path = \"01-4D-ResNet-v1-fold-2-loss-0.693.pt\"\n",
    "model_3_path = \"01-4D-ResNet-v1-fold-3-loss-0.733.pt\"\n",
    "model_4_path = \"01-4D-ResNet-v1-fold-4-loss-0.692.pt\"\n",
    "models = [model_0_path, model_1_path, model_2_path, model_3_path, model_4_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22baa27a-4146-472f-92e4-1b8b8ac95760",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hats_models = []\n",
    "idx_list_models = []\n",
    "\n",
    "for i in range(5):\n",
    "    model_path = f\"../input/resnetv1/{models[i]}\"\n",
    "    \n",
    "    # ResNet18\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], [64, 128, 256, 512])\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    y_hats = None\n",
    "    idx_list = []\n",
    "    \n",
    "    for xb, idxb in tqdm(test_dl, desc=\"Testing\"):\n",
    "        xb = xb.to(device)\n",
    "        y_hat = model(xb)\n",
    "        y_hat = F.softmax(y_hat)[:,1].cpu().detach().numpy()\n",
    "        if y_hats is None:\n",
    "            y_hats = y_hat\n",
    "            idx_list = idxb.numpy()\n",
    "        else:\n",
    "            y_hats = np.concatenate((y_hats, y_hat), axis=0)\n",
    "            idx_list = np.concatenate((idx_list, idxb.numpy()), axis=0)\n",
    "    \n",
    "    y_hats_models.append(y_hats)\n",
    "    idx_list_models.append(idx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d198bb46-27f3-433a-8586-7a8ef8ad9bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(idx_list_models[0], idx_list_models[1]) and np.array_equal(idx_list_models[0], idx_list_models[2]) \\\n",
    "and np.array_equal(idx_list_models[0], idx_list_models[3]) and np.array_equal(idx_list_models[0], idx_list_models[4]), \"Indices are not equal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c270dae-749d-4ab9-8612-1fe8e5539b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hats_avg = (y_hats_models[0] + y_hats_models[1] + y_hats_models[2] + y_hats_models[3] + y_hats_models[4]) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbbb940-6e1a-4481-b957-7c50f03eb66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame({'BraTS21ID': idx_list_models[0].astype(int), 'MGMT_value': y_hats_avg})\n",
    "df_submission.to_csv(\"submission.csv\", index=False)\n",
    "df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb1d73c-6206-4a29-a9ef-51bb79550f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4f297-3162-44e2-a3dc-9de25b3b560b",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [pipenv: Kaggle]",
   "language": "python",
   "name": "kaggle_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
