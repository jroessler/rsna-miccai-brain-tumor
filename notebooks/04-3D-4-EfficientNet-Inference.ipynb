{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c4df0b-8f20-49bc-9e75-fcc5e3985edd",
   "metadata": {},
   "source": [
    "# 4x EfficientNet Classifiers - Inference\n",
    "\n",
    "In this notebook, we will train **four** 3D convolutional neural networks and combine their predictions in order to recognize a brain tumor.\n",
    "\n",
    "For each case (i.e., person) we know whether the person suffered from cancer (1) or not (0). Each independent case has a dedicated folder identified by a five-digit number. Within each of these “case” folders, there are four sub-folders, each of them corresponding to an MRI scan, The MRI scans include:\n",
    "\n",
    "* Fluid Attenuated Inversion Recovery (FLAIR)\n",
    "* T1-weighted pre-contrast (T1w)\n",
    "* T1-weighted post-contrast (T1Gd)\n",
    "* T2-weighted (T2)\n",
    "\n",
    "I am using the datas set created by [Jonathan Besomi](https://www.kaggle.com/c/rsna-miccai-brain-tumor-radiogenomic-classification/discussion/253000#1388021). Many thanks for creating the data set!\n",
    "\n",
    "**3D images**\n",
    "* each image has the following shape: Channel x Width x Height x Depth (i.e., 1 x Width x Height x Depth)\n",
    "* *depth:* the depth represents the depth or the number of slices. (I tried various values >= 30. If for a given id, the depth <= the given value (e.g., 30), I replaced the missing depth slices with zero matrices (see [Zabir Al Nazi Nabil](https://www.kaggle.com/furcifer/torch-efficientnet3d-for-mri-no-train)))\n",
    "* added some albumentation such as CLAHE, brightness, and CoarseDropout for the training images\n",
    "* removed black pixels (see [Zabir Al Nazi Nabil](https://www.kaggle.com/furcifer/torch-efficientnet3d-for-mri-no-train))\n",
    "\n",
    "**3D CNN** <br>\n",
    "* model used: [EfficientNet-PyTorch-3D](https://github.com/shijianjian/EfficientNet-PyTorch-3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890c327-af46-4b0a-ae4f-66b0b1eb80e1",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7beefc9-1c77-469d-aa86-0852ce4d8528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "\n",
    "import cv2\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "from efficientnet_pytorch_3d import EfficientNet3D\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69032e1-e9c2-42b2-ad4b-41828b74a756",
   "metadata": {},
   "source": [
    "#### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57758aa8-de49-474a-b7b9-23fc743a24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddc7dbb-4714-42e3-b8d7-ecfcf2ad6a1f",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e95b81-d9aa-489d-94c2-f2641cbd6121",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '..'\n",
    "\n",
    "config = dict(\n",
    "    # Pre-processing\n",
    "    SLICE_NUMBER = 64,\n",
    "    REMOVE_BLACK_BOUNDARIES = True,\n",
    "    DICOM=False,\n",
    "    # Albumentation\n",
    "    RRC_SIZE = 256,\n",
    "    RRC_MIN_SCALE = 0.85,\n",
    "    RRC_RATIO = (1., 1.),\n",
    "    CLAHE_CLIP_LIMIT = 2.0,\n",
    "    CLAHE_TILE_GRID_SIZE = (8, 8),\n",
    "    CLAHE_PROB = 0.50,\n",
    "    BRIGHTNESS_LIMIT = (-0.2,0.2),\n",
    "    BRIGHTNESS_PROB = 0.40,\n",
    "    HUE_SHIFT = (-15, 15),\n",
    "    SAT_SHIFT = (-15, 15),\n",
    "    VAL_SHIFT = (-15, 15),\n",
    "    HUE_PROB = 0.64,\n",
    "    COARSE_MAX_HOLES = 16,\n",
    "    COARSE_PROB = 0.7,\n",
    "    # Training\n",
    "    N_EPOCHS = 20,\n",
    "    BATCH_SIZE = 16,\n",
    "    LEARNING_RATE = 0.001,\n",
    "    WEIGHT_DECAY = 0.02,\n",
    "    LABEL_SMOOTHING = 0.02,\n",
    "    OPTIMIZER = \"SGD\",\n",
    "    MOMENTUM = 0.9,\n",
    "    SCHEDULER = \"ReduceLROnPlateau\",\n",
    "    # Logging\n",
    "    VERBOSE = False,\n",
    "    MODELNAME = \"04-3D-4-EfficientNet\"\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6ace1-a624-45dd-a40a-18dcd36d340b",
   "metadata": {},
   "source": [
    "#### wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a67c0-30f1-4072-88e8-d89c4f346ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "run = wandb.init(project='rsna-miccai', config=config, mode=\"disabled\")\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe263350-abc7-47d3-a470-a57a7060b7e1",
   "metadata": {},
   "source": [
    "### 1. Load Data\n",
    "\n",
    "To create a 3D image, we will pick SLICE_NUMBER middle slices from each of the four MRI types (i.e., FLAIR, T1w, T1wCE, T2w). For example, if we set *SLICE_NUMBER=30*, each 3D image will have the shape: 1 x Width x Height x 30. Further, one image contains only the images from *one* MRI type as opposed to other notebooks where all MRI types are combined in a single image.\n",
    "\n",
    "* If for a given MRI type, the number of images < SLICE_Number, than we will \"fill up\" the remaining *number_of_images - SLICE_NUMBER* slices with all black images\n",
    "* We removed black pixels (for more information, see [here](https://www.kaggle.com/furcifer/torch-efficientnet3d-for-mri-no-train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eb4664-2143-4c1b-9f81-7a58d16edd73",
   "metadata": {},
   "source": [
    "### 1.1 Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2fba5-e062-4ab1-8b9e-ca72e9b36732",
   "metadata": {},
   "source": [
    "#### 1.1.1 Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c02d3-d25b-4c3f-95e5-545894a0a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.RandomResizedCrop(\n",
    "        config.RRC_SIZE, config.RRC_SIZE,            \n",
    "        scale=(config.RRC_MIN_SCALE, 1.0),\n",
    "        ratio=config.RRC_RATIO,\n",
    "        p=1.0\n",
    "    ),\n",
    "    A.CLAHE(\n",
    "        clip_limit=config.CLAHE_CLIP_LIMIT,\n",
    "        tile_grid_size=config.CLAHE_TILE_GRID_SIZE,\n",
    "        p=config.CLAHE_PROB\n",
    "    ),\n",
    "    A.RandomBrightnessContrast(\n",
    "        brightness_limit=config.BRIGHTNESS_LIMIT,\n",
    "        p=config.BRIGHTNESS_PROB\n",
    "    ),\n",
    "    A.HueSaturationValue(\n",
    "        hue_shift_limit=config.HUE_SHIFT, \n",
    "        sat_shift_limit=config.SAT_SHIFT, \n",
    "        val_shift_limit=config.VAL_SHIFT, \n",
    "        p=config.HUE_PROB\n",
    "    ),\n",
    "    A.CoarseDropout(\n",
    "        max_holes=config.COARSE_MAX_HOLES,\n",
    "        p=config.COARSE_PROB\n",
    "    ),\n",
    "    A.Normalize(\n",
    "        mean=(123.675),\n",
    "        std=(58.39),\n",
    "        max_pixel_value=255.0,\n",
    "        always_apply=True\n",
    "    )\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    A.RandomResizedCrop( \n",
    "        config.RRC_SIZE, config.RRC_SIZE,            \n",
    "        scale=(config.RRC_MIN_SCALE, 1.0),\n",
    "        ratio=config.RRC_RATIO,\n",
    "        p=1.0\n",
    "    ),\n",
    "    A.Normalize(\n",
    "        mean=(123.675),\n",
    "        std=(58.39),\n",
    "        max_pixel_value=255.0,\n",
    "        always_apply=True\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7aedeb-df7f-45a4-9646-97bde2c10c19",
   "metadata": {},
   "source": [
    "#### 1.1.2 Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec6f51-ff10-46d5-a325-e90acd1e00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicom_2_image(file, voi_lut=True, fix_monochrome=True):\n",
    "    dicom = pydicom.read_file(file)\n",
    "    # VOI LUT (if available by DICOM device) is used to\n",
    "    # transform raw DICOM data to \"human-friendly\" view\n",
    "    if voi_lut:\n",
    "        img = apply_voi_lut(dicom.pixel_array, dicom)\n",
    "    else:\n",
    "        img = dicom.pixel_array\n",
    "    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "        img = np.amax(img) - img\n",
    "    \n",
    "    img = img - np.min(img)\n",
    "    img = img / np.max(img)\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "def remove_black_boundaries(img):\n",
    "    (x, y) = np.where(img > 0)\n",
    "    if len(x) > 0 and len(y) > 0:\n",
    "        x_mn = np.min(x)\n",
    "        x_mx = np.max(x)\n",
    "        y_mn = np.min(y)\n",
    "        y_mx = np.max(y)\n",
    "        if (x_mx - x_mn) > 10 and (y_mx - y_mn) > 10:\n",
    "            img = img[:,np.min(y):np.max(y)]\n",
    "    return img\n",
    "\n",
    "def get_3d_image(mri_type, aug, dicom):\n",
    "    if config.VERBOSE:\n",
    "        print(f\"Length of folder: {len(mri_type)}\")\n",
    "    # Take SLICE_NUMBER slices from the middle\n",
    "    threshold = config.SLICE_NUMBER // 2\n",
    "    minimum_idx = len(mri_type)//2 - threshold if (len(mri_type)//2 - threshold) > 0 else 0\n",
    "    maximum_idx = len(mri_type)//2 + threshold  # maximum can exceed the index\n",
    "    if config.VERBOSE:\n",
    "        print(f\"Minimum {minimum_idx}\")\n",
    "        print(f\"Maximum {maximum_idx}\")\n",
    "    # Create array which contains the images\n",
    "    mri_img = []\n",
    "    for file in mri_type[minimum_idx:maximum_idx]:\n",
    "        # Read image\n",
    "        if dicom:\n",
    "            img = dicom_2_image(file)\n",
    "        else:\n",
    "            img = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
    "        # Remove black boundaries\n",
    "        if config.REMOVE_BLACK_BOUNDARIES:\n",
    "            img = remove_black_boundaries(img)\n",
    "            (x, y) = np.where(img > 0)\n",
    "        # Apply albumentation\n",
    "        if aug:\n",
    "            transformed = train_transform(image=img)\n",
    "            img = transformed[\"image\"]\n",
    "        else:\n",
    "            transformed = valid_transform(image=img)\n",
    "            img = transformed[\"image\"]\n",
    "        mri_img.append(np.array(img))\n",
    "    mri_img = np.array(mri_img).T\n",
    "    # If less than SLICE_NUMBER slices, add SLICE_NUMBER - mri_img.shape[-1] images with only zero values\n",
    "    if mri_img.shape[-1] < config.SLICE_NUMBER:\n",
    "        if config.VERBOSE:\n",
    "            print(f\"Current slices: {mri_img.shape[-1]}\")\n",
    "        n_zero = config.SLICE_NUMBER - mri_img.shape[-1]\n",
    "        mri_img = np.concatenate((mri_img, np.zeros((config.RRC_SIZE, config.RRC_SIZE, n_zero))), axis = -1)\n",
    "    if config.VERBOSE:\n",
    "        print(f\"Shape of mri_img: {mri_img.shape}\")\n",
    "    return mri_img\n",
    "    \n",
    "def load_images(scan_id, mri_type, aug=True, split=\"train\", dicom=False):\n",
    "    file_ext = \"png\"\n",
    "    if dicom:\n",
    "        file_ext = \"dcm\"\n",
    "    if config.VERBOSE:\n",
    "        print(f\"Scan id {scan_id}\")\n",
    "        \n",
    "    # Ascending sort\n",
    "    if mri_type == \"FLAIR\":\n",
    "        flair = sorted(glob.glob(f\"{PATH}/{split}/{scan_id}/FLAIR/*.{file_ext}\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        img = get_3d_image(flair, aug, dicom)\n",
    "    elif mri_type == \"T1w\":\n",
    "        t1w = sorted(glob.glob(f\"{PATH}/{split}/{scan_id}/T1w/*.{file_ext}\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        img = get_3d_image(t1w, aug, dicom)\n",
    "    elif mri_type == \"T1wCE\":\n",
    "        t1wce = sorted(glob.glob(f\"{PATH}/{split}/{scan_id}/T1wCE/*.{file_ext}\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        img = get_3d_image(t1wce, aug, dicom)\n",
    "    else:\n",
    "        t2w = sorted(glob.glob(f\"{PATH}/{split}/{scan_id}/T2w/*.{file_ext}\"), key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        img = get_3d_image(t2w, aug, dicom)\n",
    "    \n",
    "    # Return 3D image: ChannelsxWidthxHeightxDepth\n",
    "    img = img.reshape((1,config.RRC_SIZE,config.RRC_SIZE,config.SLICE_NUMBER))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e375fe33-206e-47fa-87ee-39e5c68ac346",
   "metadata": {},
   "source": [
    "### 1.2 Dataset and Dataloader\n",
    "\n",
    "Create a PyTorch Dataset and DataLoader **for each** MRI type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2679b9c-dcd6-435a-a228-45da0d9896b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ids and labels and make a stratified 80:20 split\n",
    "df_test = pd.read_csv(f\"{PATH}/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf7314b-c6de-4490-b3cc-ca6a5295c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNADataset(Dataset):\n",
    "    def __init__(self, ids, labels, mri_type=\"FLAIR\", split=\"train\", dicom=False, label_smoothing=0.0):\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "        self.mri_type = mri_type\n",
    "        self.split = split\n",
    "        self.dicom = dicom\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "        if split == \"train\":\n",
    "            remove_ids = [709, 109, 123]\n",
    "            self.ids = [id_ for id_ in self.ids if id_ not in remove_ids]  \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = self.ids[idx]\n",
    "        patient_id = str(patient_id).zfill(5)\n",
    "        if self.split == \"train\":\n",
    "            imgs = load_images(patient_id, self.mri_type, aug=True, dicom=self.dicom)\n",
    "        elif self.split == \"valid\":\n",
    "            imgs = load_images(patient_id, self.mri_type, aug=False, dicom=self.dicom)\n",
    "        else:\n",
    "            imgs = load_images(patient_id, self.mri_type, aug=False, split=self.split, dicom=self.dicom)\n",
    "        # Normalize\n",
    "        imgs = imgs - imgs.min()\n",
    "        imgs = (imgs + 1e-5) / (imgs.max() - imgs.min() + 1e-5)\n",
    "\n",
    "        if self.split != \"test\":\n",
    "            label = abs(self.labels[idx] - self.label_smoothing)\n",
    "            return torch.tensor(imgs, dtype = torch.float32), torch.tensor(label, dtype = torch.long)\n",
    "        else:\n",
    "            return torch.tensor(imgs, dtype = torch.float32), torch.tensor(self.ids[idx], dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf6c77-13b4-46b2-81e6-4248dcbac986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(df, mri_type, shuffle=True):\n",
    "    ds = RSNADataset(df[\"BraTS21ID\"].to_numpy(), df[\"MGMT_value\"].to_numpy(), mri_type=mri_type, \n",
    "                     label_smoothing=config.LABEL_SMOOTHING, dicom=config.DICOM, split=\"test\")\n",
    "    dl = DataLoader(ds, batch_size=config.BATCH_SIZE, shuffle=shuffle)\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65224930-558c-4db1-b28a-9de2a01bb07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAIR DataLoader\n",
    "test_flair_dl = get_dataloader(df_test, \"FLAIR\", False)\n",
    "\n",
    "# T1w DataLoader\n",
    "test_t1w_dl = get_dataloader(df_test, \"T1w\", False)\n",
    "\n",
    "# T1wCE DataLoader\n",
    "test_t1wce_dl = get_dataloader(df_test, \"T1wCE\", False)\n",
    "\n",
    "# T2w DataLoader\n",
    "test_t2w_dl = get_dataloader(df_test, \"T2w\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc05117-0a72-4c3f-ae15-629c568e0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_flair_dl))\n",
    "print(f\"Shape of the batch {images.shape}\")\n",
    "print(f\"Batch size: {images.shape[0]}\")\n",
    "print(f\"Number of channels each image has: {images.shape[1]}\")\n",
    "print(f\"Size of each image is: {images.shape[2]}x{images.shape[3]}\")\n",
    "print(f\"Depth of each channel/sequence: {images.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd7879-f56f-4241-b841-9ee3e85d46c2",
   "metadata": {},
   "source": [
    "### 1.3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a338247-fd0c-4e12-bdf8-3856b8cab0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GliobCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GliobCNN, self).__init__()\n",
    "        \n",
    "        self.efficient = EfficientNet3D.from_name(\"efficientnet-b0\", override_params={'num_classes': 2}, in_channels=1)\n",
    "        self.efficient._fc = nn.Linear(in_features=self.efficient._fc.in_features, out_features=2, bias=True)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.efficient(X)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11e1bb7-6b42-4f1e-8575-ace47e1f22ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_model = GliobCNN()\n",
    "flair_model.load_state_dict(torch.load(\"../models/01-3D-4-ResNet18-pretrained-FLAIR-roc-0.68.pt\"))\n",
    "flair_model.to(device)\n",
    "t1w_model = GliobCNN()\n",
    "t1w_model.load_state_dict(torch.load(\"../models/01-3D-4-ResNet18-pretrained-T1w-roc-0.61.pt\"))\n",
    "t1w_model.to(device)\n",
    "t1wce_model = GliobCNN()\n",
    "t1wce_model.load_state_dict(torch.load(\"../models/01-3D-4-ResNet18-pretrained-T1wCE-roc-0.59.pt\"))\n",
    "t1wce_model.to(device)\n",
    "t2w_model = GliobCNN()\n",
    "t2w_model.load_state_dict(torch.load(\"../models/01-3D-4-ResNet18-pretrained-T2w-roc-0.71.pt\"))\n",
    "t2w_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df09a3-54f9-4f8b-a083-122a50d8f565",
   "metadata": {},
   "source": [
    "### 1.4 Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb1a444-6f04-4250-9bd1-9059855f8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(net, test_dl):\n",
    "    y_hats = None\n",
    "    idx_list = []\n",
    "    \n",
    "    net.eval()\n",
    "    for xb, idxb in tqdm(test_dl, desc=\"Testing\"):\n",
    "        xb = xb.to(device)\n",
    "        y_hat = net(xb)\n",
    "        y_hat = F.softmax(y_hat)[:,1].cpu().detach().numpy()\n",
    "        if y_hats is None:\n",
    "            y_hats = y_hat\n",
    "            idx_list = idxb.numpy()\n",
    "        else:\n",
    "            y_hats = np.concatenate((y_hats, y_hat), axis=0)\n",
    "            idx_list = np.concatenate((idx_list, idxb.numpy()), axis=0)\n",
    "    \n",
    "    return y_hats, idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85b109-5ef7-475f-b4a4-5180a87e6f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_flair_hats, idx_list = submission(flair_model, test_flair_dl)\n",
    "y_t1w_hats, idx_list = submission(t1w_model, test_t1w_dl)\n",
    "y_t1wce_hats, idx_list = submission(t1wce_model, test_t1wce_dl)\n",
    "y_t2w_hats, idx_list = submission(t2w_model, test_t2w_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a56a7-1606-4062-bbca-1e93d81e6e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'BraTS21ID': idx_list, 'FLAIR': y_flair_hats, 'T1w': y_t1w_hats, 'T1wCE': y_t1wce_hats,\n",
    "                   'T2w': y_t2w_hats})\n",
    "df[\"MGMT_value\"] = (df[\"FLAIR\"] + df[\"T1w\"] + df[\"T1wCE\"] + df[\"T2w\"]) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21521e51-4b8a-4530-8013-421f88b211df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = df[[\"BraTS21ID\", \"MGMT_value\"]]\n",
    "df_submission.to_csv(\"submission.csv\", index=False)\n",
    "df_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4f297-3162-44e2-a3dc-9de25b3b560b",
   "metadata": {},
   "source": [
    "Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [pipenv: Kaggle]",
   "language": "python",
   "name": "kaggle_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
